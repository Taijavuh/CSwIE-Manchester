Algorithms and Imperative Programming - Semester 2 - 2015 Notes!

Will be a question on graphs, will need to know complexity ect. Understand dijk's alg and heuristics.
Will be a question on knapsacks, with need to know complexity ect. bnb and dp methods please!

Other two could be on any of the other course material. Could be hashing and trees, or trees/tree searching

##Â Quick Notes ##

Knapsack has a number of constraints:
	- Compatability Constraints - What task fits in what slot.
	- Scheduling Constraints - Do tasks need to be undertaken in a certain order?
	- Resource Constraints - Are resources limited or unlimited
	- Optimisation requirements - We may require solutions which maximise or minimize measures.

Heuristics:
	- These are rules we can use in decision making to help make an optimal solution easier. They can help reduce exhausive searches from exponential time to polynomial time, but may effect the results. 

Branch and Bound
	- Fractional Upper Bound is calculated by ALLOWING the fractionalisation of a item to calculate the abosolute best case.
	- Creates a tree style based on all of the possible combinations!
	- Each node contains the benefit, weight and upper bound.
	- Upper bound is maximum possible benefit at that point in the tree, calculated at each point in the tree! (using fractional knapsack as above)
	- Always work with the best possible upper bound.
	- Once a solution is found with the highest upper bound this is the best solution, we don't explore other routes with a lower benefit.

Hash Table
	- Effectively an array, normally quite big.
	- Has a hash function, converts a key to an index number that allows it to be stored in the array.
	- Good hash function makes use of all info provided by a key, uniform distribution, similar keys to different hash values, only uses fast operations.
	- Lots of different ways to handle collisions, linked list/2D array (chaining), linear probing (moving to the next index).

AVL Tree
	- We control the depth of the tree so always log(n)
	- The height must be balanced (should differ by at most one)
	- Complexity is o(log(n))
	- Use rotations to balance the tree, left/right or double rotations (left then right, or right then left)

Acyclic - No way to start at a vertex and follow a sequence of edges to return to v
Cyclic - Path from a node/vertex to it's self

Complexities:
	Djikstra: O(e+nlog(n))
	Binary Search Tree: O(n) (all ops)
	AVL Tree: O(log(n)) (all ops)
	BFS/DFS: O(N + E)
	BNB: O(2^n)
	DP: O(n * w) (where w is weight)
	Greedy: O(nlogn) (because of the sort)

A heap based priority queue:
	- There are a number of things that a heap based priority queue is made up from:
		- heap - The complete binary tree with keys that satisfy the heap-order property
		- last - A reference to the last node in T.
		- comp - The comparitor which defines the total order relation amongst the keys, the result of the comparitor should keep the minimal element at the root of the tree!
	- Inserting to a heap base PQ
		- To store a new key-element pair we need to add a new node to the tree, this is done as the LAST node in the tree.
		- Once the insertion has been completed it is possible that the heap-order property is no longer true. As a result we use UPBUBBLING

		Upbubbling:
			- Upbubbling is the process of running the comparitor of the tree on the parent and the new node. If the new node is smaller then it must be cascaded up!

	- Removal from a heap based PQ
		- To remove a key-element pair we basically have to take away the root of the tree, since this would completely screw up a binary tree we need to do something about it.
		- The process to do this is called downbubbling and ironically it's pretty much the opposite of upbubbling!

		Downbubbling:
			- Downbubbling is the process of switching the LAST node to the root element then performing comparison of child nodes in an attempt to find the highest value to switch it with until the the heap-order property is successfully validated again

The implementation of a heap based priority queue is the most efficient way to go about PQ's

## Full Notes ##
Trees

A tree is an abstract data type for heirarchical storage of information. Ironically it represents the structure of a tree, who would know!

The top of a tree is called the root!
And each element in the tree is an arbitrary element, except for the root!

Each element except for the root has a parent and zero or more children elements. This structure can be akind to that of a computer disk storage structure, or a family tree!

A tree (t) is a non-empty set of nodes storing useful information in a parent-child relationship with these properties:
	- T has a node r which is the root
	- Each node that is different from r has a parent node u
	- We can apply the rules of a family tree to that of the tree T
		- Parents = parent also called ancestor
		- Child = child also called descendant
		- Two children with the same parent are called siblings
	- A node is external (or a leaf node) if it has no children, and internal if it has one or more children.

We can define a sub-tree inside a tree where all the decendants of a specified root are part of the sub-tree!

A tree can also be ordered if a linear ordering relation is defined for the children of each node. Therefore we can define an order among them, a book could be an example of an ordered tree, where each decendant is a different chapter in the book.

The Binary Tree:
	- Probably the most key to us is the binary tree which is an ordered tree which each node has at most two children (1/0)
	- A binary tree is proper if each internal node has two children
	- For each internal node the children are labelled left/right and a left child should be ordered to come before the right so that if the tree was to be read right to left then it would read similar to a book, in the correct order.

Example:
	- We can represent an arithmetic expression utilising the binary tree where all of the external nodes are being operated on by the operation which resides within the internal node! Infact this is quite a good way for a computer to represent a mathematical expression as it can work from the bottom of the tree in a breath first format to complete the expression!

Abstract Data Type Functions:

	Accessor Methods:
		- root() - Gets the root of the tree
		Complexity: o(1)
		- parent(v) - Gets the parent node of v
		Complexity: o(1)
		- children(v) - Returns an iterator of the children of the node, empty if leaf node!
		Complexity: o(1)

	Query Functions:
		- isInternal(v) - Checks to see if the node has any children, if it does it is internal! 
		Complexity: o(1)
		- isExternal(v) - Checks to see if the node has any children, if it doesn't then it is external!
		Complexity: o(1)
		- isRoot(v) - Checks to see if the node has a parent, if it doesn't then it is the root of the tree!
		Complexity: o(1)

	Generic Methods:
		- size() - Returns the overall number of elements which are present in the tree!
		Complexity: o(1) or o(n) (this depends if the value is stored, it should be really)
		- elements() - Returns an iterator which allows us to work our way through all of ELEMENTS IN A NODE (not the nodes) stored in the tree.
		Complexity: o(n)
		- positions() - Returns an iterator whihc allows us to work our way through all of the NODES in the tree!
		Complexity: o(n)
		- swapElements(v,w) - Switches the elements at nodes v and w
		Complexity: o(1)
		- replaceElement(v,e) - Gives us the element v and replaces it with the element e.
		Complexity: o(1)

	ADT Binary Tree Accessors:
		- leftChild(v) - Returns the left child, or errors if null.
		- rightChild(v) - Returns the right child, or errors if null.
		- sibling(v) - Returns the sibling of v, or errors if the root.

Depth of a Node:

	- The depth of the node is the number of ancestors that a specific node has, not including it's self. For reference the depth of the root node is 0
	- This implies a recursive definition, ie from the node keep finding the parent and adding one until the root is located!

Height of a Tree:

	- The height of the tree is equal to the maximum depth of an external node of the tree.
	- If we were to use the algorithm previously stated the complexity would be a very poor o(n^2)
	- An alternative: determine if the node is external, if not for each of the children get the greatest of the heights from it's children.

Traversal of a Tree:

	- The traversal of a tree is a systematic way of accessing all of the nodes.
	- There are two key traversal schemes, preorder and postorder.

	Preorder
		- The root is visited first and then all of the sub trees rooted at it's children are traversed recursively.
		- Utilsing this method will produce a linear ordering of the nodes in a tree where the parents are always appearing before their children.
		- If the tree was a document this preorder would examine it sequentially.
		- Notably similar to that of a depth first search for trees!

	Postorder:
		- The compliment of preorder, traverses the leaf nodes first before visiting the root!
		- Postorder has the complexity of o(n)
		- Useful if a property requires the results from it's children before it can be computed!
		- This is notable similar to that of a breath first search for trees!

Binary Trees

	- Binary trees have a structure which enforces a specific leveling system where a level d has at most 2^d nodes.
	- A proper binary tree has a number of external nodes that is one more than the number of internal nodes.

	Preorder:
		- Can be used on a binary tree to get the value of the node then recursively call leftChild, rightChild

	Postorder:
		- Can also be used on a binary tree by recursively calling leftchild, rightchild then getting the value of the node!

	Inorder:
		- Additionally we can use an "inorder" traversal where the movement is done in the order of the tree. This is effectively visiting the nodes from left to right.
		- This can be achieved by checking if the nodes are internal and moving left through the tree until the leaf node is found, at which point we can work back up in the order left right left and traversing to the bottom always.

Data Structures for binary trees && Priority Queues!

	- We can use a different array of data structures for representing binary trees (vector-based, linked)!
	- Priority queues need sorting so the data structure we need for this needs to be implemented!

	Trees : Vector Based
		- A vector based structure is based on a simple way of numbering the nodes.
		- For every node in the tree would be assigned a fixed position in the vector! This would be calculated using the numbering function (p()) and is based off the level of the node.
		- The root would be 1, level 1 would be 2,3, level 2, 4,5,6,7
		- This is fixed for a binary tree and easy to implement!

		Complexities of the methods:
		positions(), elements() : o(n)
		swapElements(), replaceElement() : o(1)
		root(), parent(), children() : o(1)
		leftChild(), rightChild(), sibling() : o(1)
		isInternal(), isExternal(), isRoot() : o(1)

	Trees : Linked Structure
		- A linked data structure is generally only better than a vector implimentation when the tree height is large, as the vector implementation is likely to be space inefficient. ESPECIALLY if there are gaps!
		- The linked data structure is a natural way to represent binary data!
		- The linked data structure works by each node holding information about it's children

	KW: Total Order - Total order is effectively a the order of objects with keys and the rules as to how this is supposed to be attained, where the objects carry shit all meaning and the keys do all the work!

	Priority Queues:
		- A priority queue is a container of elements with keys associated to them at the time of insertion.
		- Two fundamental methods of the PQ are:
			- insertItem(k,e) where k = key and e = element
			- removeMin() removes the element P with the smallest key
		- The priority queue ADT (Abstract Data Type) is relatively simple, the elements are inserted and removed based on their ranks. In order to allow this all a comparator is defined on the creation of the priority queue which enables the keys to be measured against each other and the position of the item in the queue to be determined!

	Sorting Priority Queues:
		- PQ could be used to sort the elements in a collection, by inserting all of the elements into the priority queue and then returning them to the collection by removing the minimum would sort all of the elements.
		- But the priority queue itself needs to be sorted utilising some method:
			- Selection Sort:
				- Elements are inserted at the end of the queue and after insertion we need to locate it's position, to do this using selection sort will take o(n) whilst all other operations will take o(1)
			- Insertion Sort:
				- Before we insert the element we determine the location of the element and place it in the correct position. Similar to selection sort the insertion part is linear and takes O(n) as a result is slows down the PQ
			- Heap Sort:
				- This is the most efficient way to implement a PQ, a heap is effectively a binary tree. Thus by implementing a binary tree structure we can perform an insert with the efficiency of o(log(n))! MAGICAL!

Heaps, Dictionaries and Hash Tables

	The Heap:
		- Can perform insertions and retrievals in logarithmic time!
		- Store the elements in a binary tree instead of a sequence!

	The Structure:
		- The heap is a binary tree with a collection of keys acting as the nodes.
		- It follows a relational structure
		- There is a total order relationship on the keys.
		- Effectively the heap-order property states that for every node, the parent is less than or equal to it's children. Thus the root is always the lowest value in the structure.
		- Additionally the heap follows the complete binary tree property and the levels are propertly balanced, with a maximum of two children. (Standard rules for a binary tree)

	A heap based priority queue:
		- There are a number of things that a heap based priority queue is made up from:
			- heap - The complete binary tree with keys that satisfy the heap-order property
			- last - A reference to the last node in T.
			- comp - The comparitor which defines the total order relation amongst the keys, the result of the comparitor should keep the minimal element at the root of the tree!
		- Inserting to a heap base PQ
			- To store a new key-element pair we need to add a new node to the tree, this is done as the LAST node in the tree.
			- Once the insertion has been completed it is possible that the heap-order property is no longer true. As a result we use UPBUBBLING

			Upbubbling:
				- Upbubbling is the process of running the comparitor of the tree on the parent and the new node. If the new node is smaller then it must be cascaded up!

		- Removal from a heap based PQ
			- To remove a key-element pair we basically have to take away the root of the tree, since this would completely screw up a binary tree we need to do something about it.
			- The process to do this is called downbubbling and ironically it's pretty much the opposite of upbubbling!

			Downbubbling:
				- Downbubbling is the process of switching the LAST node to the root element then performing comparison of child nodes in an attempt to find the highest value to switch it with until the the heap-order property is successfully validated again

	The implementation of a heap based priority queue is the most efficient way to go about PQ's

	Dictionaries + Hash Tables
		- A computer dictionary is a data respository specifically designed to perform the search operation, the user will assign keys to elements and use the same keys to add/remove elements.
		- The dictionary abstract data type has methods for insertion, deletion and searching!
		- key element pairs are called ITEMS
		- We can allow multiple elements to have the same key, but in some applicatons this is probably a bad idea.

		An Associative Store
			- This is when the keys are unique and are effectively addresses for a specific object.

		Methods a ADT Dictionary Supports:
			- findElement(k) - Finds an element with a key k
			- insertItem(k,e) - Inserts element e with a key k
			- removeElement(k) - Removes an element with key k

		A simple way to implement a dictionary would be using a vector or list. This is a LOG FILE/AUDIT TRIAL method.
			- This is great if the data doesn't change much, and there isn't too much data.

			Complexity using Log Files:
				- insertItem o(1)
				- findElement o(n)
				- removeElement o(n)

		An alternative and more efficient way is using a hash table, there are two major components:
			- Bucket arrays - An array of a fixed size n, where elements are key-element pairs
			- Hash functions - A function which maps a key from a dictionary to a integer within the bounds of the array!

			Collisions:
				- It is quite likely that collisions will occur within a hash table. As a result our hash function needs to be sufficiently complex that it avoids collisions as best as possible.
				- Polynomials are a excellent choice when formulating an appropriate hash function. They tend to end up with few collisions.

			Compression Maps:
				- A compression map is used when the hash codes generated by the hash function fall out of the range of the array. The compression map helps spread out the hashed values often using a prime number. An alternative method is to use multiple add and divide.

					MAD - h = |ak+b|mod N
					DIV - h = |k|mod N

			Collision Handling:
				- Should a collision occur we need to do our best to handle it. A simple way to do this is just create a mini dictionary inside the bucket, thus allowing more than one entry per bucket array value.

			Rehashing:
				- In order to keep the load factor below a constant. We may need to change the size of the bucket array and modify the compression map. This is rehashing.

			Open Addressing:
				- Open addressing is when only one element is stored per bucket, and thus a more sophisticated method of handling collisions is needed.

				Linear Probing:
					- One way is linear probing where when we attempt to enter a item into a bucket which is occupied we calculate a new position based off the entry point and increment until we find a free space.
					- Doing this does make it hard to removeElement correctly!!

				Quadratic Probing:
					- An alternative is to linear probing is quadratic probing where the buckets attempted to iterate to are spread using ^2 to prevent clustering around a main point. However secondary clustering may occur depending on the size of the table.
					- Additonally quadratic probing makes removal operations more complex.

				Double Hashing:
					- Double hashing is a process which eliminates the data clustering all together. In the case of double hashing, should a collision occur then a secondary hash function is used to find an appropriate location. This way the clustering is avoided and the element is placed somewhere reasonable to find!

##

Ordered Dictionaries

	- In an ordered dictionary we use a comparator to provide order relation.
	- We can't use an log file or hash table for implementation of a dictionary as neither of the two data structures have ordering among the keys.
	- If a dictionary is order, the items can be stored in a vector, by non-decreasing order of the keys.
	- The ordered vector implementation of a dictionary D is reffered to as the lookup table.
	- Using a lookup table insertion takes o(n)
	- Finding an element is significantly quicker than a log file.

	Binary Search

		- If we have a ordered sequence, the element at rank i, has a key that is not smaller than the keys of the items i-1 durrr
		- This ordering allows quick searching of S
		- Doing a binary search is effectively looking in the middle, working out if it's higher or lower and adjusting the sample size until we find it.
		- A binary search runs in worst case o(log n)

	Binary Search Trees

		- We can adapt a tree data structure to use a binary search algorithm, it is effectively a tree where the left is lower and the right is higher.
		- We can perform a search at each node.
		- The computational cost of this is as before, o(log n)
		- In order for this to work we need to maintain the structure, this can be done with a balance after each insertion. (AVL Tree)

		Inserting

			- Inserting to a binary search tree is simple first we check the node is not already in the tree.
			- Then we get the node where it would be attached, perform a comparison and insert at either the left or right position depending on the result.

		Removal

			- First we find the node, if it's a leaf node we can just go ahead and take it out straigh away.
			- If it's an internal node we have to find the left most internal node in the right subtree.
			- We then copy this item up into the position of what we were removing, and move it's children if any to it's parent.

	AVL Trees

		- These trees are designed to balance themselves thus to make them as efficient as possible. Maintaining the logarithmic height of the tree, aka the height balance property. An AVL tree holds detail about the height of the tree.
		- A height balanced tree can only have the left + right side depth differ by at most one!!!
		
		Inserting

			- Inserting a key is exactly the same as for any binary tree, comparisons are performed and the element is inserted at the correct position.
			- However the insertion of the element causes the tree to become unbalanced.

		Trinode Restructuring

			- In order to balance the tree we can perform trinode restructuring on the node causing the height balance property issue.
			- This is effectively a single or double rotation on the nodes depending on the shape which moves the new node back into the correct position.

		Deleting

			- Deleting a node is as easy as before if the node is a in a leaf position.
			- If it is not we will have to perform trinode restructuring again to restore the balance.
			- To rebalance we address the height balance property issue and perform a rotato!

		woo tooobe: https://www.youtube.com/watch?v=YKt1kquKScY

##

Graphs

	- Graphs are a set of nodes and a set of edges which link the nodes together.
	- A directed graph has edges with a direction
	- A undirected graph has edges without a direction (multigraph)

	- Two nodes linked by an edge are adjacent
	- For a directed graph there is a source and target node.
	- An edge is incident if it has the node as source or target (directed) or links the node to another (undirected)
	- In undirected raphs the number of edges incident at a node is the degree of the node
	- In a directed graph the number of edges with the node as a source is the out-degree
	- And the number of edges with the node as a target is the in-degree

	- A path is a sequence of adjacent nodes and the linking edges, a path can also be empty. (undirected)
	- A path is a sequence of adjacent nodes and edges with the edges in the same direction. (directed)
	- If there is a path from A to B it is reachable
	- A path from a node to itself is a cycle (via elsewhere)
	- A loop is an edge from a node to itself (via nowhere)

	- Two nodes are connected in a undirected graph if there is a path between them.
	- In directed graphs nodes are connected if there is a sequence of adjacent nodes between them.
	- A graph is connected if all pairs of nodes are connected.

	- A subgraph is a graph with nodes and edges that as a subset of the original graph
	- A connected component is the largest possible subgraph of a graph (additional nodes cannot be added without becoming disconnected.)

	We can represent graphs in a number of ways, there are two common methods and we choose depending on:
		- Properties of the graph
		- Algorithms we wish to implement
		- Programming language + how data structures are implemented
		- Application of the code

	Methods of representation:

		Adjacency Lists:
			- Consists of a list of all the nodes
			- Each node has a list of all adjacent nodes (targets in directed graphs)
			- Complexity o(n+e) (nodes edges) - Simple stack based algo

		Adjacency Matrixes:
			- 2 Dimensional array (matrix) representing the connections between nodes in the graph.
			- Simple binary 1 if edge, 0 if not.
			- Complexity o(n^2) (nodes) - Simple stack based algo

	- Both methods are tabular representations, exact choice depends on application. Adjacency list may be array of linked lists.
	- Adjacency matrix may have a large amount of 0's and can get stupidly huge!!!
	- For undirected graphs, a matrix will be symmetrical
	- We can do arithmetic with adjacency matrices!

	Traversal:

		There are many techniques to traverse the graph:
			- DFS - Depth First Search - Visit all descendants of a node before visiting siblings
			- BFS - Breadth First Search - Visit all children of a node, then all grandchildren.
			- Priority Search - At each node choose the higher priority child first.

		Each has an appropriate data structure:
			- Stack - Last in first out - DFS
			- Queue - First in first out - BFS
			- Priority Queue - Weighted movements - Priority Search

		There is often an optimization property when it comes to shortest paths. this means there are direct methods of computing shortest paths, ie we only need to combine other shortest paths.

		This is the basis for Floyds Algorithm and Dijkstra's Algorithm

	Planarity:

		The art of depicting graphs in 2D space.

		- Embedding of a directed/undirected graph is the allocation of points and linking together via edges so that none of the edges intersect.
		- Not all graphs can be embedded in the plane!
		- Graphs that can be embedded in the plane are called planar graphs

		Graphs can also be coloured, we can determine if a graph can be coloured but it's a NP-complete problem.
		The only algorithms that exist for colourability are exhausive unlimited back-tracking algorithms and run in exponential time.

		The four colouring of planar graphs is a celebrated problem. "Every planar graph can be coloured with just 4 colours." who cares really....

##

Problem Solving Techniques

	Algorithmic techniques give a range of tools for constructing algorithms which may or may not help.

	Three basic techniques:

		- Divide and Conquer
		- Greedy
		- Dynamic Programming

	Divide and Conquer

		- Ironically this method involves splitting the input into parts and then solve them, once done combine the solutions back together again to give the final results.
		- Divide and conquer fairly often uses recursion to enable it to be implemented quickly and effectively on large data sets.
		- Quite a wide array of algorithms use divide and conquer:
			- Sorting algorithms (mergesort/quicksort)
			- Fast integer multiplication
			- Fast matrix multiplication
			- Nearest neighbour problems

	Greedy

		- A greedy strategy is one where in effect it grabs the best possible result presented to it first. It will always go for what may appear to be the best initial result and work the rest out in the same manner to solve the problem.
		- It may not always work
		- It won't always be the most efficient way
		- It will be the most efficient way in some cases
		- It will normally be quite quick 
		- Examples:
			- Path-finding algorithms
			- Job scheduling
			- Spanning trees
			- Knapsack!

	Dynamic Programming

		- Always produce an optimal solution to the problem
		- Creates an array with results for all the possible sub problems
		- Bottom up style, solves all the similar problems and combines to answer the given problem.
		- Examples:
			- Path finding
			- Text similarity
			- Knapsack!
			- Optimal search trees
			- Travelling salesperson problem

	Polynomial Time:

		- Both divide and conquer and greedy algorithms often will give a fast (polynomial time) algorithm.
		- Dynamic programming can be fast but it also used when no fast algorithms are avaliable.
		- An algorithm with polynomial time is when worstcase complexity is among:
			o(1) o(n) o(n^2) o(n^3) ect
		- We consider these as fast algorithms

		Exponential Algorithms
		- Algorithms which behave in worst case 2^n n^n or n! are very slow and are considered intractable algorithms, in general!
		- They run so slows as the size of N increases and they are only going to get worse!

	What can we use?

		- Working out how complex a computational task is very difficult. Problems which may appear very similar has have widely different time complexities.
		- Exponential algorithms arise from either exhausive enumeration, listing everything!!! or systematic searches using unbounded backtracking!

	Colouring a graph:

		Enumeration
		- To colour a graph with each nodes linked having a different colour, we need a minimum number of colours, this is the chromatic number.
		- One approach is exhaustive enumeration: 
			- Enumerate all the allocations of k colours to the nodes of the graph
			- Check the allocation to see if it's valid

		As a result of this there will be k^N allocations of k colours to the nodes. And to check we will need E checks

		Thus the complexity is o(E*k^n) not great.

		Systematic Search
		- For each node we try the colour, if all the colours fail we undo the colour of the previous and try a new colour.
		- We keep doing this until all the nodes are coloured.

		This is exponential in the worst case.

		But all known algorithms for graph colouring are exponential! Thus it is a NP-Complete problem, the only algorithms known are exponential.

	The Travelling Salesperson Problem:

		- Find a tour of N cities in a country where the tour should visit each city once, return to the starting point and be of minimum distance.
		- This is an optimisation problem.
		- We can solve by exhausive enumeration, thats exponential.
		- Or dynamic programming!
		- But they are all exponential in the worst case...

	Knapsack:
		- The knapsack problem requires us to fit a number of items of different sizes into a container optimally by value.
		- Some knapsack problems have polynomial and some only have exponential solutions!
		- Knapsack has a number of constraints:
			- Compatability Constraints - What task fits in what slot.
			- Scheduling Constraints - Do tasks need to be undertaken in a certain order?
			- Resource Constraints - Are resources limited or unlimited
			- Optimisation requirements - We may require solutions which maximise or minimize measures.

		The problem is allocate the tasks satisfying constraints.

		Examples of scheduling:
			- Multitasking operating systems
			- Industrial processing
			- Rostering
			- Timetabling

		Scheduling:
			- Can have linear or polynomial solutions, can use simple allocation methods.
			- Most scheduling problems only have exponential solutions:
				- We may end up using exhausive enumeration

	Heuristics:

		- These are rules we can use in decision making to help make an optimal solution easier. They can help reduce exhausive searches from exponential time to polynomial time, but may effect the results. 

	Puzzles:

		- Interestingly most puzzles can only be solved with exponetial algorithms, thats what makes them interesting. We generally have to use exhausive search with unbounded backtracking!


	Things still to touch on:
		- Knapsack in detail
		- Djikstra in detail
		- Fractional Knapsack
		- Textbook 

Before Exam Watch:

	https://www.youtube.com/watch?v=WN3Rb9wVYDY - DJK
	https://www.youtube.com/watch?v=EH6h7WA7sDw - Knapsack



