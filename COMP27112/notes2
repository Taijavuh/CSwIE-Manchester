Computer Graphics COMP27112 - Semester 2 - 2015

##

Quick Notes:

3D Graphics:

2 Types of display: Raster + Vector

Modelview Matrix - Used for transforming geometry drawn
Projection Matrix - Used for transforming the camera image onto the screen
P = Projection * Modelview * Pspecified (open auto does this)

System is a producer consumer with CPU Software making pixels and GPU Frame Buffer eating pixels.

Uses double buffering, switches buffer in the VBLANK time.

We can simulate camera movement by transforming the object instead.

ROTATION MATRIX:

About X: cos, -sin, sin, cos
About Y: cos, sin, -sin, cos
About Z: cos, -sin, sin, cos

Homogenous Coordinates (The addition of w) - Needed to keep a consistent matrix representation, it's a 4th dimension. Means we can produce composite matrixes.

Scan Conversion = Sweep Line!

We can string together rotations to create a composite transformation matrix.

To transform about a vector V, we map to two axis's (to x, to y) then transform and then inverse the translations and move back.

Surface Normal = Cross Product (x1*y2)-(y1*x2) ect
Dot Product = (x*x) ect

Modelling Transformation - Models the scene
Viewing Transformation - Maps the camera to the world.
Projection Transformations - Transform the objects onto the plane basically setting a lense (Parallel / Perspective Projection)
Clip to View Volume - Creates a Cuboid (parallel) or Frustum (perspective) which define near + far planes.
Perspective Division - Converts x,y,z,w to x,y,z 3D points
Viewport Transformation - Converts 3D to 2D but retains Z for hidden surfaces.

Illumination:
	
	Local - Treat each separately - NO Shadows, NO Object interaction
	Global - All objects together + effecting each other.

	Diffuse Reflection - Absorbtion + Uniform Re-Radiation
	Specular Reflection - Reflection to air/surface, same colour as light source
		Perfect = Mirror
		Imperfect = Shiny

		(0 = theta)

	Lamberts Law: I = Icos(0)

	Ambient Illumination - I = K*I
		- Where K = Ambient Reflection Co-efficient
		- Where I = Intensity of ambient light

	LIM 1: I = K*I

	Diffuse Reflectivity - I = Ikcos(0) OR Ik(N.L)
		- Where k = Diffuse reflection co-efficient

	LIM 2: I=K*I+Ik(N.L)

	Distance Travelled - I = I/k+kd+kd^2
		- Where d = Distance

	LIM 3: I=kI+(I/d)*k(NL)
		- Where d = k+kd+kd^2

	Finally Incident + Wavelength - = 1/2(sin2(y-u)/sin2(y+u) + tan2(SAME))

	LIM 4: I = k(a)I + I/d(k(d)(N.L)+k(s)(R.V)^n)
		- Where k(a) = Ambient Reflection Co-efficient
		- Where I(a) = Intensity of Ambient Light
		- Where I(d) = Intensity of Light Source
		- Where d = k(constant) + k(linear)*d + k(quadratic)*d^2 (d = distance)
		- Where k(d) = Diffuse constant
		- Where k(s) = Specular refleciton coefficient

	Colour = do intensity for RGB (3 times)

	Gouraud Shading - Use interpolation to smooth out discontinuities. (does average colour between points) may miss edges

	Mach Banding - Seeing the lines between different polygons.

	Phong Interpolation - Computes normals for each pixel and completes illumination model for each.

	(pixel bigger) Bilnear Interpolation - Compute a texel from adjacent texels.

	(texel bigger) MipMap - Have a set of texture maps for scaling down, smaller polygon/model = less detail. (maps made in pre-processing)

	Bumpmapping - Alter the surface normal by two vectors to give a new value. Often produced as a texture, different intensity = greater modification to normal.


Image Processing:

	3 Resolutions:

		Spatial - Size (no pixels)
		Brightness - (no colours/grey) amplitude
		Temporal - No images captured+stored (fps)

	H - Hue - Underlying colour
	S - Saturation - depth of colour
	V - Value - Intensity of sample/brightness

	CIE - Range of colours realisable by a monitor showen by the triangle.
	RGB - Make any colour but hard to predict the changes.
	HSV - Colour space created.

	Extrinsic Parameters - Camera to World Transform
	Intrinsic = Image Camera Transform

	Four Connectivity = north esw
	Eight - Block around but not always

	When Resampling
		Bilinear interpolation - similar to 3d - Sum of nearest neighbours.
		Nearest Neighbour - only takes one into account

	Thresholding:

	Incremental - Work through the image with a suggested threshold from the histogram, if not averaging at the end, increment threshold and go again.

	Automated - Intial threshold, if average done use it. Otherwise keep going.

	P-Tile Thresholding - Assuming an object is brighter/darker than the background and we know it occupies percentile P of image. Set the threshold so 1/p values are below the threshold.

	Modal Thresholding - Perform thresholding with the value between the histograms two peaks. May need to apply smoothing

	Optimal Thresholding - Find average intensity levels for foreground + background peaks, and set the threshold to the average of the two.

	Double Thresholding - Use two different algorithms and then compare for overall thresholding.

	Issues: Local thresholding methods are invoked where illumination is varient. Divides the image into regions which are thresholded independently of it's neighbours.

	Color + Thresholding - Threshold for each colour space. Better to use HSV for this.

	Edge Detection:

	Canny Deriche = Gaussian followed by Sobel/Robert. Reduces noise before feature extraction.

	Region Detection:

	Take ariel photo, thresholding to clearly define the object, region detection to find dark pixels, maybe estimate average size for grouping detection, use templating to determine if object is correct shape (effectively on a car, see if the item has a white no plate or headlights or wheels)

	Convolution:

	For Smoothing: Gaussian: Lessens the effect of noise, smooths down curve of histogram.
	For Edge Finding: Sobel: Important for recognising regions, significant change in intensity, two kernels to put toegether to find gradient + orientation of line.
	For Templte Matching: Uses the template to determine the measure of similarity between the object and the template thus finding matches.

##

Interactive Computer Graphics:	

OpenGL:
	- Has a 3D geometry setup
	- Therefore 3D trigonometry needs to be used
	- Homogeneous matrices (x,y,z,w)

A scaling matrix has the x,y,z setup we would expect and a 1 at the end.

Mesh Data Structures:
	- Has a vertex list
	- A edge list, which indexes into the vertex list
	- A face list, which indexes into the edge list

	All together this forms a list of meshes.

F0 = E0 = V0 - > V1
	 E1 = V1 - > V2
	 ect

3D Viewing Pipeline:

	- Using a 3D vertex, there is a 6 step pipeline which results in a 3D image being simulated on a 2D pixel px, py
	- 1 - Modelling Transformation
	- 2 - Viewing Transformation
	- 3 - Projection Transformation
	- 4 - Clip to view volume
	- 5 - Perspective division
	- 6 - Viewport Transformation
	- Output

The Camera

	- OpenGL uses a camera and perspective projection in order to display the image
	- A good way to invision this is a stage area where if an item either leaves the front or the back of the stage it will become invisible. The edges of the stage are called the near plane (projection plane) and the far plane, we can define these values to give a different appearance to our work.
	- We can also define the left, right, top and bottom positions of the projection (near) plane (glFrustum)
	- This gives us a viewport from a point defined by the "eye" position of the camera.

Diffuse light/surface interaction

	Diffusion:
		- Occurrs if a surface has pigment particles which will redirect the light.

	Lamberts Law:
		- Lightsource Ip
		- Effective intensity recieved is Ie
		- Law is - Ie = Ip cos(theta)
		- The helps us calculate how intense the light should be

	Reflection:
		- When reflection occurs the light is specularly reflected with no change of colour.
		- As we drift away from the exact opposite angle of incidence then the amount reflected reduced

	Fresnel Equation:
		- To model this the Fresnel Equation is used where F is the fraction of light reflected
		- See notes...

	Shading + Interpolation

		Gouraud Interpolation:
			- Computes average vertex normals at ABC to compute the overall gradient

		Performing Texture Mapping:
			- We can sample the texture at each pixel and apply the texture as part of the rasterization process.
			- We can also apply filters at this stage, if required.

		Bump Mapping:
			- Bumpy surfaces look bumpy because the surface normal changes across the bumps, this means that the top of the bumps appear brighter than the sides.

Image Encoding:

	- Image encoding allows us to reduce the size of photos without reducing the quality significantly, this is done through established standards, ie jpeg, which encodes it in a certain format of which it can decode again to a similar quality. 
	- There will be differences in the two images but they will not be significant.

Raster Graphics Displays:
	- Raster graphics uses a 2D array of pixels (screens) or dots (printers)
	- Therefore images must be sampled in order to be represented
	- The more samples we have a better fidelity, but a sample is always an approximation.

Vector Graphics Displays:
	- Vector graphics were the first type of graphical display, these effectively draw a line from A to B
	- Vector displays are usually monochrome.

A basic graphics architecture would be:
	- CPU runs the application
	- Graphics software on the CPU performs transforms, conversions, shading, textures, hidden surfaces
	- GPU Holds the frame buffer memory and the DAC to render to screen
	- The CPU produces pixels to the frame buffer memory and the DAC consumes the pixels
	- Double buffering is a method creating two frame buffers, this allows them to be read/written two alternatively, ensuring that syncronisation is achieved.

OpenGL
	- There is an alternative to OpenGL - it's DX11/9/8/7/6/5/ whatever
	- They offer roughly the same functionality.
	- OpenGL is the graphics system and sits between the application and the input devices + display
	- OpenGL provides a specification of an API for functions that perform 3D computer graphics.
	- It's used in engineering, games, education. Everything.
	- It's evolving, opengl 1-2 have a fixed pipeline, opengl 3 has a programmable pipeline
	- OpenGL can be used on a crazy number of languages on pretty much everything (yay for everything)

	Things about OpenGL
	- OpenGL generates pixels, it doesn't handle interaction devices. This is why we add GLUT, which is a library for openGL that does this.
	- OpenGL does 3D graphics, coordinate transformations, a camera, removes hidden surfaces, lighting, texturing, and pixel operations.
	- OpenGL expresses coordinate transformations as 4x4 homogeneous matrices, we can transform in many directions.
	- We can combine and nest transformations, thus creating complex objects from simple parts.
	- The camera creates 2D scenes of 3D scenes.

	Libraries
	- Using both GLU and GLUT as libraries for OpenGL we can perform higher level rendering.
	- GLU provides functions for curves, surfaces, cylinder, discs, quadratics ect as well as utility functions for viewing, textures, tessellation.
	- OpenGL requires convex polygons (if a line was drawn between two points and fell outside the shape it would be non-convex!)
	- To allow us to draw shapes that satisfy this we can use tessellation.
	- GLUT provides interaction, and primative shapes.
	- The combination of GLUT, GLU and OpenGL is loosely called OpenGL

Transformations
	- Objects are represented with 3D cartesian coordinates as a point in space
	- A vector represents a direction in space, with respect to X Y Z axes. It has a length.
	- A relationship between an object and the origin will have a vector.
	- We can apply geometric transformations to points to change them, we can perform:
		- Translation
		- Scaling
		- Rotation
	- To transform a shape we just modify all it's points.

	Translation:
	- Translation is simply the addition of a set value t to the new position, acting as a 3D shift.

	Scaling
	- Scaling applies a scale factor enlarging the shape, this is effectively multiplying the shapes coordinates by a set value.

	Rotation:
	- Rotation is slightly more complex and requires the use of cos and sin.

		To rotate about Z:
		x = x.cos(theta) - y.sin(theta)
		y = x.sin(theta) + y.cos(theta)
		z = z

		X:
		x = x
		y = y.cos(theta) - y.sin(theta)
		z = z.sin(theta) + z.cos(theta)

		Y:
		x = x.cos(theta) + x.sin(theta)
		y = y
		z = z.-sin(theta) + z.cos(theta)

	As each of these transformations are different we need a homogeneous solutions ( the same for each )

	To do this we utilise vector and matrices, to give a uniform way to perform transformations on objects.

	A scaling matrice would multiply each axis in it's own column to give the desired result.

	Rotation using a matrice simple allows us to put the cos, -sin, sin, cos pattern in each position to give a 3x3 transformation matrice.

	Translation is a pain!!!! 

	Translation requires a 4x4 matrice as we don't want to modify any of the co-ordinates by multiplication. As a result we need to add a 4th row and column to allow us to add the translation which will be * 1.

	This also allows us to do projection from 3D to 2D.

Homogeneous Coordinates

	- The form (x,y,z,w) is called homogeneous coordinates.
	- 'w' is the 4th dimension! But we don't care why...
	- The homogenous coordinates allow us to perform all of the translations inside a single matrix.

Composite Transformations

	- Because we are using homogeneous coordinates we can apply two transformations at once
	- To do this all we do is perform the transformations onto the matrice in the order we wish to carry them out.
	- It is vital that we do this in the correct order!!!
	- If we were to add two matrix inverses in a composite transformation, the result would do nothing when applied. You would also be able to see nothing was going to happen when looking at the composite transformation.
	- Should note that not all matrices actually have an inverse.

Transformations in OpenGL

	- In OpenGL every 3D point to be drawn is automatically transformed by the projection matrix and the model view matrix. These enable camera and shape positioning and the way that the camera image is projected onto the screen.

Vectors:

	- We can perform normal vector transformations on vectors in OpenGL
	- Vector geometry is quite important to 3D graphics, it's based around it...
	- Cross Product - y1*z2 - z1*y2, x1*z2 - z1*x2, x1*y2 - y1*x2 - IE each by each, minus the opposite.
	- Dot Product - x1*x2 + y1*y2 + z1*z2 - Opp NO by Opp NO for result!

##

Polygons & Pixels

	2D Graphics Units is the Pixel
	3D Graphics Units are the Polygon

	A polygon is an ordered set of vertices, with a set of edges between each pair of vertices. The polygon is bounded by the vertices.
	We can have bizzare complex polygons but we avoid these.

	Planar Polygon - When all vertices lie on a plane. This is ideally what we want.
	Non-Planar Polygons - Don't lie on a single plane, "have two sides" - These can cause issues with graphical algorithms. (Most graphic systems will only guarantee to draw planar polygons)

	Polygon Attributes:
		- Winding - The order in which the vertices connect is the winding.
		- Faces - If we have a consistent winding then the polygon has a front and a back face.
		- Surface Normal - Vector perpendicular to the plane of the polygon. Gives the polygon a distinguishable front and back, this can bue used to give a distiguishable front and back, and describe it's orientation in 3D space. (important for lighting/collisions/culling)

	We can use the cross product to find the surface normal by finding two sequential edges, inverting one and computing the cross product.

	Polygon Soup - Have a list of all the polygons and colour and draw individually.
		- It's a waste of storage space.
		- A loss of semantics (what does the polygon belong to?)
		- Brute force rendering, makes interaction complex!

	Polygon Meshes - A linked group of polygons used to represent surfaces
		- Reduces storage as vertices and edges are SHARED
		- Makes interaction easier
		- Allows us to structure individual models.

	Meshes:

		Triangle Strips:
			- Collection of linked triangles
			- Very widely used - efficient as we are only adding 2 verticies to create a new triangle!

		Triangle Fan:
			- Collection of linked triangles again this time in a fan like structure.

		Quad Strip:
			- Same as linked triangles but with quads, obviously requires more vertices.
			- Tesselated into trinagles during rendering

		Quadrilateral Meshes:
			- Collection of linked quadrilaterals
			- Used in terrain modelling for approx curving
			- Quite effective for a 4x4 grid or anything which isn't a strip
			- Tesselated into triangles during rendering.

	Once we have modelled in 3D space we utilise a camera to create a 2D screen image (via the viewing pipeline)

	Scan Converting:
		- The process of converting the true geometry of a line into pixels.
		- Samples the geomerty and appropximates the nearest pixels avaliable.

	Bresenham's Algorithm is an application of scan converting, (y = mx + c | y = y + m) y gets rounded.

	Scan converting a polygon is slightly trickier, and there are many approaches.
		- We can plot the xy position in the same way as for plotting a basic line
		- Then fill the area covered
		- This is not really used, there are more efficient methods.

	The "sweep line" algorithm
		- Steps down a pair of edges filling in everything inside the edges or covering over half of the pixel.
		- EASY!

	Hidden Surface Removal:
		- We need to make sure things which are technically behind something else aren't being shown.
		- We can solve this in two ways, either in world space or display space.

			World Space:
				- This means that we are working the issue would in 3D geometrically. This is really hard and was the first approach!

			Display Space:
				- This means whenever we are generating pixels we check to determine if another 3D object closer to the eye maps to P, a much simpler and logical approach!

		The Z Buffer:
			- z Buffer holds the z-value of each pixel.
			- Effectively holds information about what COULD be displayed by P, if there are multiple objects in P's line (or whatever)

		Z Fighting:
			- If the z-buffer has a lack of precision then incorrect rendering may occur, which probably looks awful. This is stitching.
			- A solution is glPolygonOffset()

	Structures:
		- A model is made of objects
		- Each object is made of surfaces
		- Each surface is made of polygons
		- Each polygon is made of edges/vertices!
		- A simple structure to create big complex things!

	General Polygon Mesh + The Mesh Data Structure

		- Provides a flexible way to define linked polygons.
		- The Mesh Data Structure:
			- Face List - Has a list of all the faces which leads to the edge list
			- Edge List - Has a list of all the edges which leads to the vertex list
			- Vertex List - Has a list of all the vertexs

	In general meshes are often big, and can have many different file formats. It's a very complicated area and techniques like laser scanning are ways that potentially we can generate meshes very quickly and easily.

##

Cameras

	Viewing in 2D
		- We need to specify what we want to see and where we want to see it.
		- We are effectively mapping the world to our screen, much like taking a photograph.
		- A window in the world is translated to the viewport on our screen - This transformation is called the viewing transformation, it's a simple process which involves scaling the shape to be the same size as the viewport.
		- Clipping is when we want to remove parts outside the viewport
		- Multiple Windows - We may have multiple windows displaying multiple viewports, giving a different on screen arrangement.

		Viewing Transformation: P(screen) = M(view) * P(world)

	Viewing in 3D
		- In 3D graphic we need to convert our 3D view to a 2D view before it can be displayed. This is a much more complex operation.
		- We need to set the following transformations:
			- Modelling
			- Viewing
			- Projection
			- Viewport
			By performing transformations we are effectively performing the operation of arranging a scene, pointing the camera at the scene, adjusting the zoom and determining the size of the image.

		Modelling + Viewing:
			- They have a certain level of duality, if we are adjusting the camera distance from the object or the object distance from the camera, the relative position remains the same.
			- This is how we do it, as we don't have a camera and moving the camera x units away, we can achieve the same effect by moving the object 3 units in the opposite direction!
			- A camera has an eye point, a centre of interest and a up vector. We can use this information to apply transformations to all the objects!
			- We then make the viewing transformation and apply it, thus giving us the same view as if we had a real camera.

##

Projections

	The process of projections performs the mapping from the 3D coordinates to 2D coordinates.
		- Parallel Projection - Used in CAD and Engineering Drawing for precise measurements.
		- Perspective Projection - Used due to their sense of realism.

	Parallel Projection:
		- The planes are directly projected onto the plane at the point where the projectors intersect the projection plane thus angles between edges may be distorted.
		- Orthographically when inline with the axis x,y,z these are idea as the lengths/angles will not be distorted, which makes them ideal for engineering drawing.
		- We can use a number of popular projection planes: Isometric, Dimetric, Trimetric
		- Projections can make ANY ANGLE
		- Projection plane can have ANY ORIENTATION

	Persepective Projection:
		- Reflects the way we see things.
		- Has a centre of projection "eye point" so the projectors converge
		- Objects further away become smaller, edges become distorted.
		- Things could be either small OR far away
		- There are 3 "classic" perspective projections based on the number of "vanishing points" are in the image
			- 1 point - 2 of xyz Parallel to the projection plane
			- 2 point - 1 of xyz Parallel to the projection plane
			- 3 point - 0 of xyz Parallel to the projection plane

		We can compute perspective based on where the projection plane is located by working on the point where the vector from the object to the eye point intersects the plane, once we have computed the perspective projection we also need to divide by w, which is PERSPECTIVE DIVISION.

	"The Camera" can only see objects infront of it, in it's FoV and up to a finite distance.

	We define the view volume which is attached to the camera, it creates a projection (near) and far plane which are the boundaries for displaying images in the world.

	In parallel projection the near and far planes are orthogonal to the camera and they are a cuboid.
	But in perspective projection the view volume is a frustum, which is a truncated pyramid, defined by the near and far planes.

	We have an issue when we do perspective as we lose the idea of the 3D object's z-coordinate. As we want to keep this information for hidden-surface removal it's kinda important.

	As a result we use projection normalization which we distort the object and apply an orthographic projection, which gives us the same results as a perspective projection.

	Clipping is a result of when a object falls outside the regions to be displayed, it is therefore clipped out.

	The Viewport Transformation:
		- Now we have 3D coordinates, we then perform a transformation from -1 to 1 in x+y and then retain Z to determine how to remove hidden surfaces.

##

Rendering

	Local and Global Illumination
		- A local illumination model effects objects directly thus each object in a scene is separate from other objects
		- A global illumination model effects objects directly AND indirectly and thus each object is also effected by other objects

	Illumination Models
		- The interaction of light is complex and our methods behind this are approximate.
		- (We are only ever approximating)

		Elements - To develop a model step-by-step we need to include the following:
			- Ambient Illumination
			- Diffuse Reflection
			- Positional Lightsource
			- Specular reflection
			- Colour of lights and surfaces

		Reflectivity - Three kinds of reflection
			- Perfect diffuse reflection - Some light is reflected in all directions the result is a dull or matte surface
			- Perfect specular reflection - Light is reflected perfectly the result is effectively a mirror
			- Imperfect specular reflection - Light is reflected imperfectly thus some of the rays are at irregular angles, the surface looks shiny with highlights.

		We can model the different effects.

		Illumination Sources:
			- Ambient Illumination - Multiple reflections cause a general level of illumination in the scene.
				- Each object is uniformly illumniated, so there's no 3D information in the image!
				- It's not the same as true ambient lighting
			- Point Illumination - Source at infinity
				- We can use lamberts law to calculate the light falling on a surface
				- We also have a diffuse reflection coefficient of the surface
				- This gives us some depth to our scene
			- Point Illumination - Source in the scene

		We can also apply a light source distance based on the way that light intensity falls off with the square of distance travelled.

		Modelling Specular Reflection:
			- We need to work out an appropriate way to calculate specular reflection.
			- We can work out based on the divergance of the angle from the primary specular angle.
			- We can use phong's specular function to calculate this (effectively cos against angle)
			- Taking into account wavelength is the fresnel equation
			- By doing this we can work with a point light source, and diffuse/specular reflection

	Once we have a local illumination model we want to implement it!

	Shading a Surface:
		- Flat/Constant Shading
		- Gouraud/Intensity Shading
		- Phong/Normal-Vector shading

	Flat Shading:

		Simplest approach, compute the colour at a vertex and use it for ALL the pixels in the polygon. This means that each polygon is uniformly coloured based on it's orientation and we can clearly view the mesh!

		Mach Banding - Effectively is the fact that we can really easily see the edges of the strips because they stand out.

	Gouraud Shading:

		Invented by Henri Gouraud in 1971, uses interpolation to smooth out discontinuities. A method that approximates the underlying surface by averaging the normals where vertices are shared.

		Effectively uses scanlines and works out what corners are what colour then gradients the middle (average colors)

		It gives a better result which is much more smooth. BUT it is still polygonal (no natural smoothing)!

		Problems:
			- Specular highlight may be distorted or averaged away as shading works on VERTEX colours.
			- Might still see mach banding

	Phong Interpolation:

		- Phong interpolates normal vectors and computes the illumination model for EVERY pixel!!
		- This means that the light source can clearly be seen on the shaded model. (Basically the highlight is correct!)

		BUT

		Local illumination model takes about 60 flops to compute a colour for a pixel.
		A gouraud shaded triangle is 180 flops (2 per pixel)
		Phong is 60 PER PIXEL!!!

##

Surface Detail

	Texture Mapping
		- Texture is a 2D array of texels (often read from an image file)
		- Defined in it's own coordinate system, conventionally refferred to as (u,v)
		- We associate the texture coordinates with each vertex, and then interpolate the texture coordinates during scan-conversion.
		- The pixel colour is blended with the texture colour (sampling the texture at each pixel)

		Seams - Textures will often have edges which are where they repeat, often good to have repeating textures so they match up and are seamless.

		Mismatches - We can have situations where the pixel resolution is > texture resolution and the otherway around, we need to filter the texture. It's effectively possible that two pixels will map to the same texel.

		Solutions if the pixel res is >

			No Filter - Select the pixel we would (image looks blocky)
			Bilinear Interpolation Filter - Computes a texel colour from adjacent texels averaging both horizontally and vertically. Effectively smooths the image.

		But if the texel res is >

			We may end up with images which are considerably blurred because the texels being matches are so far apart! (thus missing detail)

			Mipmap Filtering:

				We can use mipmapping which uses a simple idea that the further away we are the less detail we need. We use a set of texture maps and select which map to use based on the distance.

			Creating a MipMap

				We effectively downsample by 1/2 until 1x1 texture, we store all the textures in memory.
				When selecting a textures we do it according to the distance of the pixel from the view point.
				We can also choose the two closest textures and do bilinear interpolation

			When using mipmapping we are effectively replacing aliasing with blurring.

	Textures as Illumination
		- Textures can be used to add accurate illumination to a real time scene.
		- Off-line we compute accurate diffuse illumination of the scene and use a global model.
		- In real time apply lightmap textures and actual surface textures.

	Modelling Bumpy Surfaces
		- The surface normals change across the bumps so the top is brighter than the sides.
		- So instead of altering the surface colour we can alter the surface normal.
		- Introduce "bump" vectors to add to the surface normal and alter it's direction.

	We can draw a texture as a "bump map"
		- Value of a text can be a value to control bumpiness
		- Can use values to compute gradients.

##

End of Part 1

##

Image Processing + Computer Vision

	Image Processing - Manipulating a digital image to generate a second image.
	Computer Vision - Extracting numerical or symbolic information from images

	OCR - Recognise characters in an image, printed text, hand printed text, cursively written text ect.
		- We can quite easily read printed text
		- We can kinda read hand printed text
		- We haven't yet found a way to read handwritten text reliably

	Biometry - Identifying people from their physical characteristics. Basically the recognition using images of the physical characteristics. A key area for the future + now is the iris! Lighting causes the iris to dilate/contract thus changing the size. This can be mapped to coefficients which are unique for each individual.

	Ariel Photography - We can take photo's of the earth looking down, these give us a view with coordinates and a upward direction we need to track. We can also assemble these into a larger mosaic thus computing a overview of the area.

	A computer vision system performs 3 key stages:
		- Enhancement - Performs an operation which removes any degredations/unwanted data that may cause issues extracting the features.
		- Feature Extraction - Identifies structures in the image which are useful for the objects being analysed.
		- Feature Recognition - Uses the features extracted to assign a label to the images/parts of the image.

##

Image Representation

	The Human Visual System
		
		Works by light reflected from surfaces focused onto light sensitive cells in the eye. This converts light energy to electrical signals which are transmitted to the brain. These are the objects we see. The eye also contains structures to regulate the amount of light and focus rays of light.

		The eye is proposed to be most sensitive to RGB colours, and this is how we often represent colours in a computer.

	Image Capture

		Many instruments exist to capture images. Mostly captured by video cameras. All instruments convert energy to a visible form. The key is that continuous data will be transformed into discrete sampled image.

		Sampling is a process which maps the image captured to a value, then quantisation is used to convert this to the nearest integer which can be represented. Quantisation effectively does create some noise. We measure quality by reviewing the signal to noise ratio.

		Image degredation can occur in a number of ways:
			- Noise in electronics
			- Geometric distortion via lense
			- Scattering by material in optical path
			- Overflow/Clipping happens if a signal has an excessive amplitude

		Guassian Noise has zero mean and finite amplitude, it causes corruption in that any pixel will have a value that differs from the expected one.

		SNR = 20log10 sig/noise

	Image Resolution

		Three Key Resolutions:
		- Spatial - No. of pixels
			- Doesn't take into account distance of objects from camera / fov
			- Nyquist rate - Can't tell much if we have under the rate of pixels.

		- Brightness - No of shades of grey / colours in image
			- Effectively increasing the number of different shades we can have with progressively more.

		- Temporal - No of frames captured/processed per second
			- Number of images which are capturing analogue video. Realtime processing requires 12 (min)

		Interactions

		- Spatial and brightness resolutions need to be considered separately as they have different effects. But they can interact in affecting the overall perception of an image. Sometimes poor spatial resolution can compensated for with a good brightness resolution.

	Connectivity + Distance

		We need to decide if a pixel is connected to another in digital images we need this to detect lines ect. To avoid making the wrong decision we need to be flexible in the choice of connectivity model. (either a four or eight connectivity) Thus choosing which is the most appropriate in the given situation.

	Colour Representation

		Colours can by synthesised to create suitiable combinations of RGB primary colours. These can be combined to make almost any colour. RGB is convienient as colour images are captured in this format, but it's probably not the best. (if we change a value by a fixed amount it depends on what it was prior), it's also hard to predict what will happen to a colour if we change it by a certain amount.

		HSV is an alternative - Hue (underlying colour), Saturation (depth of the samples colour), Intensity (Intensity of sample/brightness)

	Camera Calibration

		First stage is to relate the image co-ordiantes to a frame aligned to the camera. Second is to relate that camera frame to a arbitrary co-ordinate frame at that is fixed to a point in the physical world. There are two sets of parameters (extrinsic/intrinsic) that relate the camera to the world co-ordinate transform.

		The extrinsic parameters define the translation + rotation of the camera co-ordinate frame so that it aligns with the world co-ordinate frame.

		The intrinsic parameters perform correction to ensure that there are no optical aberrations.

##

Image Transforms and Feature Extraction

	Can be defined to all operations which manipulate image data, from simple operations to more complex ones.

	Often propose image transformation algorithms.

	Feature Extraction - Any operation that extracts information from an image.

	Image Processing May:
		- Remove/Correct Degredations
		- Improve the appearance of the image
		- Identify + Quantify structures in the image
		- Transform the image to an alternative representation.

	Point Transformation - Manipulate the Grey/Colour of a pixel without considering the neighbouring values. (Brightness + Colour)

		Grey Scale Manipulations: May be manipulated in many ways to modify the image's apperance. May use a constant multiplier, may add a constant value.

	Thresholding - Transforms an input image into a binary image, thus either 1 or 0. Helps significantly for making decisions regarding the identity of objects or pixels.

	Histogram Manipulation - Transforming based on the histogram, likely to achieve well defined and well founded results. Works based on the frequency of each value in the image. Then we expand the more common values and compress the smaller to create a more uniform histogram.

	Local Transforms - Computed using a small region of interest surrounding the pixel. 

	Convolution - Template that is placed over all possible image locations, at each location the product is computed and the output value is given at that location. The use of convolution with a template can be used to "repair" images, or convert them.

	Smoothing - Removing sharp, sudden changes in the brightness function. May be cause by noise, or obuscuring objects. We get a new image with an improved signal to noise ratio. But this will result in some edges not being as sharp as they were previously.

	Sharpening + Edge Enhancement - Converse of smoothing. Sharpening exaggerates them. Edges are important as humans detect edges much easier. Edges define structure! Effectively working by looking for adjacent similar pixels

	Global Transforms:

		- Hough Transform - Initially for detecting linear features in an image, it's used to locate line in an image by firstly enhancing the edges in an image then thresholding them.
		- Geometrical Transformations
			- Linear Transformations
				A geometric transformation will move a pixel from x,y to x1,y1, they are related by the transformation equation.
				We can apply the transformations as before with a single matrix product.

			- Non-Linear Transformations
				Involves higher order terms with polynomials. Allow this.

			- Image resampling
				If we are rotating an image to a position where it is unlikely to have a non-integer position we may need to resample the image
				We can truncate, round or share the pixel.
				- Nearest Neighbour Interpolation - Rounds to the nearest neighbour.
				- Bilinear Interpolation - Weighted sum of the four nearest neighbours. More computations, but more pleasing output.


