Operating Systems

	The Basics

		Process - A program that is currently in execution. It's not a program on the disk.

		Address Space - Memory locations which are usable by the process.

		Thread - The sequence of instructions obeyed.

		Process = Thread + Address Space

		Multi-threading - Many threads of execution which are within the same process.

		Several programs can be in memory at the same time.

		The OS may pause a program swap it out of memory and restore it later, this is an effective application of relocation.

		An OS can provide a "Virtual Machine" which is a convenient abstraction allows the program to think they have utilisation of the hardware all by themselves.

		OS has two execution modes:
		- User Mode - Normal programs and applications run in user mode.
		- System/Privileged/Supervisor Mode - Operating system components run in system mode.

		The following are protected in user mode:
		- Memory Operations
		- CPU Allocation
		- I/O Operations
		- File Operations
		- Network Operations

		Applications can utilise protected resource by performing a System Call this is like a method and operates much like this:

		- Accessor for System Mode
		- Calls OS routine that is required.
		- Parameters are checked
		- Action performed
		- Returns to User Mode

		Most programmming languages do no allow system calls to be made directly, however the standard libraries will often feature prewritten methods which allow the system calls to be carried out.

		An operating system is effectively partitioned in to loads of little pieces which effect the operating system in their own way. 
		IE: File Manager - Looks after the file on the hard drive.
		Memory Manager - Allocates and deallocates memory space and keeps track of what memory is currently in use.
		Process Manager - creation, deletion, CPU allocation
		Network Manager - Handles network traffic ect
		User Interface - GUI, shell

		The running of an application from the command line would effectively be something like this:

		- Read command
		- Find program file
		- Allocate
		- Read the fle into memory
		- Find appropriate libraries required for execution
		- Execute
		- Finish "cleanly"

		Monolithic System - Where all components are not architecturally separate and thus the system is effectively just one massive mess.

		Layered approach keeps a structure to the operating system, at the bottom is the hardware and near the top is the UI. The layers work by accessing the functions/operations and services of lower layers. The "kernel" sits right at the bttom and has probably the most fundamental functions for resource management.

		Large operating systems tend to have issues (complexity/debugging) thus are very large.

		Microkernels keep very minimal functionality in the OS, hence the name.

	The Process Manager

		The Process Manager supports the use of "Processes and Multi-Processing" (Programs which are executing - Many programs) as well as "Threads and Multi-Threading" (Sequences of instructions to be executed - Many sequences)

		Time-Sharing: All processes share a CPU and PC the process believes it is executing on it's own virtual CPU whilst the real CPU switches between processes thus to process many programs at once.

		The action of process switching keeps the CPU busy.

		The operating system is effectively a collection of processes.

		Thread = ABSTRACTION of instruction-sequence obeyed by CPU

		How the Process Manager works:

			- The system is started
			- A running process wishes to start a new process
			- A process-creation system call is executed
			- The parent process creates a new child process

		In Unix parent processes are associated throughout.

		A process can be terminated in a multitude of ways:

			- Normal Exit (As Expected)
			- Error Exit (Program Exited but something went wrong during execution)
			- Fatal Error (Error caused program to exit)
			- Killed (Unix kill 234)
			- It is also possible for the parent process to terminate.

		Process State Machine

			Newly created processes start as "Ready"
			Ready -> Running
			IF (waiting for I/O or Event) -> Blocked
			ELSE -> Ready (we can assume in this instance the operation was interrupted/time-slice expired)
			Blocked (I/O or Event Happens) -> Ready
			IF (terminated/finished) -> Exit

			3 KEY States - Ready, Running, Blocked

			The PCB (Process Control Block/Process Descriptor)

			This is a table maintained by the operating system with information on all the processes.

			- Process ID
			- Parent ID
			- State (Saved Registers)
			- Memory Management
			- File/I/O Management
			- CPU Scheduling
			- Accounting
			....	

		Threading

			Pro's
			- Threads are faster to create than a new process.
			- Are great on systems with multiple CPU's
			- Allows the process to do something even if part of it is blocked.

			Con's
			- They all share one address space.
			- They all need PC, Registers, Stack
			- They all share code/files ect
			- Harder to code

			Threads can exist on two levels: User-Level and Native/Kernel Level

			The difference between the two is:

			- User Level - Threads exist within the process and thus the creation is swift. However the threads only exist on one CPU!

			- Kernel Level - These threads exist so that the OS creates and schedules the threads, this method is good for multiple CPU's and to prevent the entire process from blocking however the creating is slower as a system call must be made.

		Scheduling

			The scheduler is a component of the process manager and it makes the decision as to what "ready" process should be run next.

			There is one scheduler per CPU (core)

			The scheduling is carried out by a specific scheduling algorithm, of which there can be a few of.

			This applies at both process/kernel level threads

			CPU Burst: Process is Executing on CPU
			I/O Burst: Process is Blocked waiting for I/O

			CPU Bound: Process has Long CPU Bursts
			I/O Bound: Process has Short CPU Bursts

			A long CPU burst will keep other processes waiting.

			Non-Preemptive Scheduling: Process runs until terminated or blocked

			Preemptive Scheduling: Process can run for a fixed period of maximum time. Once this time is reached it is interrupted and set to be ready so another process can be run. This requires a timer interrupt which can be called a time-slice or time quantum.

			Preemptive Scheduling Methods:

			First Come First Served (FCFS)

				Simple algorithm, whatever process is in the ready state first runs first until it is blocked or finished.

				Requires a single queue.

				When the CPU is free take the item from the head of the queue.

			Round Robin (RR)

				Simple algorithm, often used in time-sharing systems, improves averages for waiting/turn around.

				Works by running the queue one unit of time at a time thus each process gets an equal allocation of processing time.

				Requires a single queue.

			Shortest Remaing Time First (SRTF)

				This algorithm is not time-sliced but is preemptive.

				For each newly ready process it will check if it's CPU burst is less than the time to complete the current process's CPU burst. If this is the case, the process active with be terminated and the new process will be run.

				An issue with this algorithm is STARVATION, it is quite possible that using this algorithm a process may be overlooked many times if it has a long period of execution.

			Non-Preemptive Scheduling Methods:

			Shortest Job First

				As you would expect this method simply chooses the shortest job to run first. IE the process with the shortest CPU burst in the queue.

				This minimises average turnaround/waiting

			Priority Scheduling

				Using process priority will allow us to run first/last or allow longer/shorter time slices and help ease the issues of starvation.

				A separate policy and mechanism is likely to allow priorities to work better in a modern system.

				Static Priorities - preDetermined for each process. (externally defined)

				Dynamic Priorities - assigned by the system to achieve a set goal. (internally defined)

				A system can use a combination of both.

			Having Multiple Queues

				Potentially having multiple queues can help the priority scheduling work:

				Higher priority queues which must always run first.
				Higher priority queues get more processor time than lower priority.
				Processes may move between queues dynamically.

			Multilevel Feedback

				Another way to order the queues using priorities is to assess how they utilise the time that they are allocated, by starting all the processes in the same queue and those that do not use their quota get moved up whilst the opposite gets moved down. AS they move down the queue they are assigned a greater amount of execution time (quanta)

				This is know as ordering by bound (higher priority for I/O bound, lower priority for CPU bound)

			Another Way

				A even more complex way of handling process scheduling is to run queues with a different algorithm. IE Q1=FIFO Q2=RR Q3=priority

				In Q3 each process is given a quantum which gets reduced by 1 per clock tick when the quantum reaches 0 it is removed from the CPU.

				And then basically the process with the highest quantum gets priority.

			Disadvantages:

				It's not so great at scaling, boosting I/O bound processes is less desirable.

			There are plenty of other process/thread scheduling algorithms, real time systems may have their own algorithms which meet the priority of the task. IE: earliest deadline first.

		Process/Thread Syncronisation

			All this information applies to both threads and processes!

			Data Inconsistency - Disagreement about data values

			Synchronisation - Using policies and mechanisms to ensure the correct operation of cooperating processes.

			Critical Section/Region - Section of code which shared data is utilised.

			Mutual Exclusion (mutex) - At most on process can be in the critical section at one time. Thus ensuring data inconsistency cannot occur

			Semaphores

			P(S) - WAIT

			V(S) - SIGNAL

			S is intialised as the number of processes allowed in the critical section at once, this is normally one.

			P() Adds a process to the queue and frees the CPU up.

			V() Takes a process from the queue and makes it ready to execute.

			Deadlock

			A collection of waiting processes, each process is waiting for something which can only be provided by another of the processes which are waiting!

			An example of Deadlock could be the dining philosophers!

			Message Passing

			If data is to be shared, semaphores are a good way to control access.

			If no shared data we can send copies to other processes!


	Java Threads

	extends Thread or implements Runnable

	use class.start() to start which initialises the run method.

	Can throw interruptedexceptions!

	The synchronized declaration can be made which will aquire a lock before the method can start.

	A synchronized block can also be added ie

	synchonized {

	} // sync

	A lock can be relinquished temporarily using 

	void wait()

	And threads can be notified the lock has been relinqiushed by using

	void notify()
	OR
	void notifyAll()

	Any object can be explicitly notified.

	Multiprogramming

		Loading multiple programs which have been used in the past and switching between them. It's the ability for code to be relocated (position independent code)

		Issues:

			Programs could write to each others memory. Protection is needed.
			Partitons are smaller than main memory.
			Provides illusion of n concurrent programs, but limited to n.

		Relocation

			Doesn't provide any protection. Programs can still write to any memory.

			A solution: Introduce base and limit registers before the program executes! Addresses added to the base MUST be less than the limit.

		Memory Management Unit (MMU)

			The MMU is hardware that translates logical address provided by the CPU and converts it to a physical address in the main memory.

			The MMU sits between the processor and the memory system. It uses a table to translate from a logical to a physical address. This is called the page table, and it's maintained by the operating system.Must be blisteringly quick as all the addresses are routed through it.

			CPU -> MMU -> Cache -> Main Memory -> Disk

		Virtual Address

			Converting an address generated by a program to the actual memory address! 

			Undertaken by the MMU!

			Virtual memory is provided to allow a processor to address a larger address space than is implemented by physical memory.

			And thus to support the operating system in management of processes. Shown as the logical addresses to the CPU. 

			There are two key methods for implementing Virtual Memory:

				Paged Virtual Memory
					- Fixed size, often 512-64kb

				Segemented Virtual Memory
					- Variable size 1 and 2^16 or 2^32 bytes

		Paged Virtual Memory

			Virtual Address space is divided into a number of equal-sized pages (blocks of memory)

			The virtual address SPACE is given by the number of bits in the address bus of the processor.

			If a processor uses 32-bit addresses the virtual address space is 2^32 or 4GB.

			The physical address space is then divided into a smaller number of page frames. Thus as a result a page frame is the same size as a page.

			Often the physical memory will be smaller than the virtual address space! It might be the case that only 256mb is avaliable when the virtual address space is 4GB.

			Number of pages = address space / page size

			Number of page frames = address space / block size

			A logical address generated by the processor is split into two bit fields. The page number and the offset, 

			Number of bits in page number = log2(nv)

			Number of bits in offset = log2(block size)

			The processor wil then generate a logical address and the page number field will allow the MMU to see if the page is in memory or not. 

			If it is in memory, a physical address will be computed by replacing the page number with the page frame number.

			If it's not in memory, the transfer is aborted (page fault) and the operating system will load the page from disk to the memory.

			Page Faults:

				When a reference is made to a page that is not in memory, a page fault will occur.

				- The current process will be halted.
				- The operating system will cause the page to be loaded from disk into a page frame. (In a multitasking operating system other processes would be able to run)
				- When the page has been loaded the memory reference will be tried again.

			Page Replacement:

				- When another page needs to be loaded one of the pages already loaded will need to be overwritten.
				- An algorithm will need to be used to determine which page should be replaced.

				Least Recently Used (LRU)

					Applies the "if something has been used recently then it probably will be used again, and the reverse"

					A way to implement this would be to have a timestamp counter with each page which is updated whenever the page is accessed.

					However doing this cannot be done efficiently in memory. As a way to reduce this, we can add a counter to each TLB entry. However this counter is expensive and can triple the size of each page table entry!

				First in First out (FIFO)

					We can use a sequence number to tag pages! FIFO can then find the oldest page and remove it from memory!

					However it might be a critical bit of code!

				Not Recently Used (NRU)

					Instead of the least recently used mechanism this combines it with the principle of finding the pages which are used the least!

					Works by having two bits:

					R - Referenced
					M - Modified

					Then at intervals the clock interrupt triggers and clears the referenced bit. All the bits that are referenced in the interval are marked. Any that are left at the end are left as a candidate for removal!

				Pre-Paging:

					If a process which has been swapped out is being brought back we can be efficient by keeping track of the key pages in the process and load these parts in one go before we restart the process, this would reduce the chance of page faults!

			Page Change:

				When the processor writes to memory the contents of the page will change.

				But! When should we write back?

				For each page table entry we have a dirty bit, this is set by the processor when the page has been written to. This means that when the page is unloaded from physical memory it needs to be updated on disk.

				If it hasn't been written to there is no point in updating the disk, this takes a long time! Thus we wish to avoid this where possible.

			Other Page Flags:

				Page Resident - If the page is currently in physical memory or not.
				Page Used - If the page has been used yet.

		Segmented Virtual Memory

			The segements can be of various sizes, think like and orange, might be close, might be miles of difference.

			Segementation is better for supporting the operating system as a whole!

			Each segement can be viewed as an address space in its own right.

			A segement has two key attributes:
			- Access Rights - Determines what programs can use the segment.
			- Usage Rights - Determines what operations can be performed on the memory. (Much like permissions read-only ect..)

			Segments:
			- Ensure processes do not interfere with one another.
			- Ensure the operating system has control of the computer.
			- Prevents programs written by users taking over the computer.

			A segment address is divided into two fields:
			- Segment Number (determined by processor architecture)
			- Offset

			The segmentation process works very similarly to paged virtual memory!

			Except with segement faults instead of page faults, and gets the segment number ect.

			Obviously with segmentation the operating system needs to focus more on managing the positioning of segments in the memory to ensure that the correct decisions are made when placing segements into the physical memory to ensure maximum efficiency!

			This is where external fragmentation can occur. Where memory space is wasted due to 'holes' in the physical memory!

			We could avoid external fragmentation by shuffling segments to make free space in memory. But this requires extensive copying of data and takes a significant amount of time.

			An alternative would be to use an algorithm to determine where to place the segment in memory! As the operating system maintains a list of the addresses and sizes of the holes. We can use an algorithm:

			- Best Fit - Scans the list and determines which best fits the segment.

			- First Fit - Scans the list until the first hole is found which fits the segment.

		So whats the difference:

			Paging - Cannot be seen by a programmer, replacing blocks is trivial, disk traffic is efficient, and was invented to simulate large memories.

			Segmentation - Is programmer visible, block replacement is hard, there can be external page fragmentation, disk traffic is not always efficient, and the process was invented to provide address spaces of varying size.

		Swapping

			Fixed partitioning wouldn't allow 20 programs running at the same time. Thus memory images can be moved in and out of disc.

			Fragementation may get messy and we will need to compact memory. Real programs have dynamic memory allocation, swapping programs out singally is fairly easy and wasteful. 

			We need to be able to handle multiple programs and do relocation + protection.

		Shared Memory

			Sometimes we need to share memory between processes
			- Shared files
			- Shared data space (unix pipes)
			- Shared library code (dll's)

			Page tables can be set up so that virtual addresses in multiple processes can translate to the same real page. This can be organised by the OS but allowing processes to alter their own page tables could be a security issue!

		Security

			Page table entries can contain permission information (R,W,X) this can be used as access control: 
			- Code can be marked 'read only'
			- Page tables can be marked 'read only'
			- Data can be 'read/write' but not 'execute'

			However pages are not natural units of protection!

	Controlling Input + Output

		A computer system is likely to interface with a wide variety of input and output devices. An I/O module has an interface to the system and controls one or more I/O devices.

		An I/O module contains special purpose hardware for performing transfers between the device and the processor or main memory.

		The processor controls the I/O device by writing to a special purpose register provided by the devices I/O module. They will have unique addresses within the system.

		I/O devices have a status register, this is a 1 or 0 and it indicates the status of the device. For example in a keyboard, 1 means a character has been typed and the value can be found in the data register, 0 means no character has arrived since the last time the processor read from the data register.

		The data register would hold the value typed at the keyboard.

		These registers would be mapped at a perticular address within the computer system.

		Polling with I/O - Assuming the computer has nothing to do with will spend it's time polling the keyboard! IE consistently checking for changes.

		In practice this is not a viable concept! Whats the point in the computer waiting for keyboard entry!

		An alternative would be for the process to be called occationally. However:

		The more frequently the code is called more time is wasted BUT more responsive.

		The less frequently the code is called less time is wasted BUT less responsive!

		The solution: Interrupts!

		There are two types of interrupt:

			Software Interrupt - When the processor meets a special interrupt instruction or when an error has occured.

			Hardware Interrupt - Processor has a number of external connections called interrupt lines which can allow the device to signal to the processor it wants to interrupt!

		Interrupts have a wide variety of uses, the focus will be on a hardware interrupt generated by an external device.

		If the keyboard has a connection to the processors interrupt line our keyboard will send a interrupt when it has a character ready to send.

		As we are using interrupts there is no reason to read the status register, thus we can just read the data register! Every time the a character arrives the processor will be interrupted and then resumed. There is no time wasted with this approach!

		The processor will also send a type of memory read called an IACK (interrupt ACK) at this point the device which interrupted the processor will respond with a value to indicate it's identity.

		The processor then saves the value of the PC, and registers to memory. The interrupt device is then looked up in a table to find the starting address for the device. This finds the ISR (Interrupt Service Routine) which contains the code that handles the type of interrupt, the starting address of this is loaded to the PC.

		The code is then executed until the return from interrupt point is reached and the interrupt ends. The processor reloads the PC and the program that was executing continues as if nothing happened!

		This is a great and efficent way to get values from an I/O device. But interrupts are even more useful when we are dealing with mass storage devices!

		If we write to disk normally, it's possible that it may take up to 15ms before the sector is loaded.

		When writing to disk a process called Direct Memory Access (DMA) can be utilised:

			- Processor inform the disk to DMA I/O by writing to a command register in the DMA I/O device.
			- DMA controller starts write process.
			- Process gets on work another work until...
			- DMA device is finished, which is signalled by an interrupt.

		Effectively what this does is allows the procesor to continue working whilst the disk is not ready. Eliminating a polling scenario where the CPU is consistently waiting for the disk to be ready.

		Interrupt-driven I/O is more efficient than programmed I/O, less time is wated.

		BUT both schemes rely on the processor to transfer the data and the data rate is limited by the processors ability to read/write to/from the I/O device.

		For large volumes of data DMA is performed.

		DMA is optimised for tranferring blocks of data into the memory system. The DMA will handle the transfer of a whole block of data without processor intervention! This means the processor can do something else until it is interrupted!

	File Manager

		A file is a collection of related information on secondary storage, ie data/programs (.java, .c, .h, .class) bleh, it's structure is dependent on it's type, but often a sequence of bytes.

		A file can have attributes such as size, last update, owner and can be opened/read/written to/closed/deleted

		O/S should recognise some file types based on extension, and in unix files have a magic number at the start of the file!

		Access can be sequential and process the file in order from start to end. 
		Or can be direct, processing the data in any order from start to end!

		File System:

		Files have ID's - 
		SFID - System File ID - Exists for the lifetime of the file.
		UFID - User File ID - Exists for the lifetime of the process!

		The file system will need to make system calls work on files:
		open: file-name -> UFID
		read: UFID & count -> data
		write: UFID & data -> 

		These are multiple OS layers
		- Naming service
		- Storage service
		- Disk driver

		Directory: file-name -> SFID

		A file is a leaf in the file tree!

		Data Structures

			Each process has:

			- Working Directory 
			- UFID
			- File Attributes
			- Where to find data on disk
			- File information table in memory

		File Structure

			Contiguous Blocks (CD/DVD)

			- File has a file start block and a block count.

			This is simple and fast, but it means that fragmentation may occur and there will be seeks required to move to new blocks inbetween user requests.

			List of Blocks

			- File has a pointer to the next block in the current block. 0 = EoF

			This is random access and is very slow! But doesn't suffer from fragmentation!

			List of Blocks 2

			- File has a pointer to the next block in a File Allocation Table!

			This means that the table can hold free block info aswell, however it does require the table to be cached in memory!

			List of Blocks 3

			- File has a separate data-structure detailing it's blocks!

			This means only one table will be needed in memory, proportional to the file size.

			This is UNIX's inodes, which contain the file attributes, 11-15 block no's and links to blocks containing more block numbers.

			inodes are kept in a separate partitioned disk area!

		Issues

			Concurrency - How should multiple accesses be coordinated?
			- 1 writer, many readers?
			- application defined?

			Performance
			- Use of cache
			- efficiency dependent on algorithms/types of data.
			- RAID striping

			Access Protection
			- R/W/X permissions (chmod)
			- Access Control Lists

			Recovery
			- Consistency Checking
			- Partitions
			- Journalling
			- Mirroring

		Virtual Memory and Storage

			Virtual Memory and File Managers both copy info between RAM and Disk.

			A unified VM/FM allows memory-mapped files.

			open - map file to Virtual Address
			read - access virtual address, page fault causes disk read
			write - access virtual address, page rejection causes diskwrite
			close - unmap page

			Potentially usefull as allows programs and libraries to share easily, but could have different access patterns.

	Case Studies:

	Windows XP

		Aims
		- Portability - Written in C and C++ and Hardware Abstraction Layer - Processor Dependent
		- Extensibility - Layered architecture; executive in kernel mode.
		- Compatibility - DOS emulation in a virtual DOS machine.
		- Performance, Scalability, Multiprocessor Support, Reliablity, International Support

		Unifying Themes

			- Good for non-programmers
			- Windows appears as a single entity.
			- GUI built into OS
			- Win32 API hides library/system split

			It's object orientated
			- Handles
			- Methods

		Registry exists - effectively replaces scattered configuration files.
		Hard to copy, share, backup. Heavily used, single point of failure!

		Scheduling is prevemptive with priorities, starvation prevention and extended quantum of foreground application.

		Memory management allows processes to share pages. Uses a version of LRU. Always keeps some pages free so a new page can be loaded!

		NTFS has a Master File Table with meta data and block start/count.

	Linux

		Shell - Allows user-level process, excutes programs it's a CLI.

		Can fork processes, permissions inherited from parent!

		I/O can be redirected using the pipe system call!

		User Level - Applications ect
		Programmer Interface - System programs, System calls
		Kernel Level - Managers/Drivers

		Processes have unique PID and address space, with User/Group ID aswell. Scheduling priorities are implemented.

		Virtual Memory is paged, LRU with second chance implementation.

		iNode keeps information on file and block pointers!

		Files are protected using RWX for owner, group, other!


