Distributed Systems - COMP28112 - June 2015

Key Topics - From Past Exam Papers
	- Byzantine Generals
	- ACID
	- Two Phase Commit
	- Ahmdahls Law
	- Littles Law
	- Lamport Clocks vs Vector Clocks
	- Denial of Service Attack
	- RPC mechanisms (why can't RPC handle pointers?) (client/server stubs?)
	- Caching vs Replication
	- Bully Algorithm
	- Elastication
	- What is an IDL?
	- What is the RMI registry?
	- Why can't we sync clocks?
	- Why assume latency is zero is a fallacy?
	- Mutual Exclusivity!
	- Semantics
	- Load balancing + expansion

Topics are addressed by passing through the lecture notes in order

A distributed system is a computed platform which is built with many computers that:
Operate concurrently, are physically distributed, are linked by a network and have independent clocks

A key quote relating to distributed systems is that of Leslie Lamport:

You know you have a distributed system when the crash of a computer you have never heard of stops you from getting any work done!

The consequences of a distributed system are that of:
Concurretn execution of processes
No global clock
No global state
Units may fail independently


So? Why do we bother having distributed systems?
People are in many cases distributed but need to work together in order to achieve a common goal.
Hardware needs to be physically close to people who are distributed
Information is distributed but needs to be shared
Hardware can be shared by allowing work to be done in parallel.

Loads of existing distributed systems:
Internet, intranet, email, dns, electronic banking, p2p, mobile computing

Evolution:

Parallel computing was a hot topic in the 70s and 80s

Some of the earliest distrubuted systems were that of airline reservation systems and banking systems.

The real upgrade was that of networking technology and the world wide web!

There are 8 key fallacies of distributed computing.
All of these are FALSE and will cause big trouble and painful learning experiences!

The network is reliable
Latency is zero
Bandwidth is infinite
The network is secure
Topolgy does not change
There is one administrator
Transport cost is zero
The network is homogeneous

1 : The network is reliable
- Hardware can fail, maybe have power failures.
- We may need to buy redundancy hardware and invest in software which can retry, ack, reorder, error correct messages!

2 : Latency is zero
Latency/Ping - Time taken for the data to move from one place to another measured in time! Limited by speed of light. AT least 30ms from EU to NA and back.
- Make a minimal number of calls and transfer as much data as possible within these calls!

3 : Bandwidth is infinite
Bandwidth - How much data you can transfer over a period of time ( bits / second )
- Constantly grows but so does the amount of information we are attempting to squeeze through it, and it can be reduced by packet loss!
- We need to compress as much as possible to ensure that the bandwidth is not exceeded!

4 : The network is secure
- Quite obviously the network is rarely completely secure, and it's going to be common to want to build security into applications from day 1!
- It's possible that as a results of security considerations you may not be able to access parts of networked resources, different user accounts may have different privileges!

5 : Topology doesn't change
- The topology doesn't change when we are in the lab, but once we get out the will often be servers appearing and disapearing without warning!
- We can't rely on specific endpoints or routes and need to be able to consistently be prepared for change. Abstraction of the physical structure of the network helps with this, IE DNS names over IP's

6 : There is one administrator
- In general there will be different administrators associated to a network with a very varied degree of experience. As a result it may be hard for them to locate problems!
- We need to take into account the coordination of the upgrades on a network to ensure that the human factor is not overlooked!

7 : Transport cost is zero
- Going from the application layer to the transport layer is not free!
- Information needs to be serialised to get the data onto the wire.
- The cost for running a network is NOT zero and we may be required to lease the necessary bandwidth ect!

8 : The network is homogeneous
- It's quite possible in the modern world for a linux and windows pc to connect!
- Interoperability will be needed!
- Using agreed standardised technologies such as XML json ect this allows us to share data quickly and easily!

We have a number of challenges in distributed systems:
Heterogenity
Openness
Securit
Scalability
Failure Handling
Concurrency
Transparency

Increased Performance
- There are a huge number of applications where a good response time is needed!
- We have the oppertunity to increase execution time by using more computers to work in parallel! But there are limits to doing this!

There are a number of scientific operations which are inherently parallel, and in a number of ways machines can operate on different data at the same time.
In an ideal situation the exection would be sped up be a factor equal to the number of processors but this is unlikely to be true!

Parallel Computing is multiple CPU's in the same computer
Distributed Computing is computers that are connected by a network

In a number of cases it is unclear where we draw the defining line between the two! A number of problems are common between both but have slightly different flavours to them!

Key Issues:
Communication - Computers need to send data to each other!
Synchronisation - We may have issues keeping the data in sync if we allow one process to modify something whilst another is doing so!

NOT ALL APPLICATIONS CAN BE PARALLELISED!!!!!!

Sometimes it's just not appropriate to perform parallelisation as it's not possible to distribute the tasks needed!

Amdahls law!

It's used to find the maximum expected improvement to an overall system when only part of the system is improved!
It is used to predict the theoretical maximum speedup using multiple processors, the law takes into account the parts of the program which are serial and can only be executed by one processor/thread and that of which can be executed in parallel. (We probably should consider the law of diminishing returns!)

The best way to go about parallelise an application would be to:
- Identify instructions which can be executed in parallel. (These really need to be independent of each other)
- Identify instructions which are excuted on multiple data, thus different machines can operate on different data!
- We can schematically plan out dependencies!

There are a large number of challenges when attempting to parallelise a system. We need to consider all of them!

Architectures:

Tightly coupled:
	- Machines which are highly integrated that appear to act much like a single computer!

Loosely coupled:
	- Client-Server
	- Peer-to-peer
	- Web services
	- Distributed objects

Tightly coupled systems:
	- Distributed Shared Memory (DSM) - This gives the illusion of a single shared memory unit. It spares the programmer the concerns of message passing! 
		- However machines are still connected by a network and as a result they are subject to the bandwith and latency constraints!
		- It may be hard to keep track of the location of shared data
		- There may be delays when attempting to access remote data
		- Additionally there are potentially issues with concurrency over shared data!
		- The data needs to be replicated on multiple machines, there must be a form of coherence!

Loosely Coupled Styles:
	- Layered Architectures - The layered style is much like a sandwhich with the data flowing through every layer in between!
	- Object-based architecture style - This style has a number of objects and runs method calls in between them where required!
	- Event-Based architecture style - This style has an event bus which allows components to remove and re-add items as required.
	- Shared-Data space architecture style - Similar to the event based but much more free, the data space simply holds the content and it can be delivered/published as required!

Middleware:
	- Middleware is a software layer that provides a programming abstraction to mask the heterogeneity of the underlying platforms (networks, languages, hardware)
	- EX: Java RMI, CORBA 
	- However the end-to-end argument implies that some aspects of communication support cannot always be abstracted away from applications.

	The End-To-End Argument:
		- The function in question can completely and correctly be implemented only with the knowledge of the application standing at the end points of the communication system. Therefore providing that questioned function as a feature of the communication system itself is not possible!
		- It's true but probably written by an asshole....

Client Server Model:
	- Server
		- Passive (Slave)
		- Waits for requests
		- On a request it handles it
		- Can be stateless or stateful
	- Client
		- Active (Master)
		- Sends requests
		- Waits for and recieves server replies

Peer to Peer Model:
	- Pretty much data is just transferred between the two participants in the system!

Client Server vs P2P:
	- CS is widely used, has a specification, is asymmetrical, is centralised and scales poorly!
	- P2P is symmetrical, is truely distributed, can share over a large number of participants, resource discovery is a challenge!

Variations on a theme:
	- We may need to use multiple servers to increase performance and resilence
	- We may need to use mobile code
	- Users may require low cost machines
	- Mobile devices are likely to be added and removed!

Quick Point - Mobile Code:
	- A mobile device may download and applet from a server which it then executes it's self without need for communication with the webserver.

Quick Point - Thin Clients:
	- A thin client will often use a server to perform complex computation rather than relying on local resources!

When designing a system which is to be distributed we must consider:
	- Performance
		- Responsiveness
		- Throughput
		- Load balancing
	- Quality of Service
	- Caching and Replication!
	- Dependabilty
		- Correctness
		- Security
		- Fault-Tolerance

All of the models have the key ingredients needed for a distributed system, and we need to be aware that some are more appropriate than others in certain situations. They are all abstractions of reality and we need to be sure to handle:
	- Process Interaction
	- Failure
	- Security

A number of failures in distributed systems are down to the poor management of concurrency! There are a few reasons for this:
	- Performance of communication channels
	- Computer clocks desync/timing events

A synchronous system waits for a process to be completed before continuing, as a result:
	- Execution speeds have a upper and lower bound
	- There is a bound on the transmission delay
	- The clock drifting has a known bound

On the other hand a asynchronous system is a mysterys and all of the above elements could take a arbitrarily long time!

Key Points:

	Failure in a distributed system is normally down to the management of:
		- Concurrency
		- Failures
		- Security

RPC - Remote Proceedure Call

	- Distributed systems can be built to send and recieve methods, these are rather low level and basic.
	- RPC provides a higher level alternative.
	- It also provides access transparency, use of a local service has the same form as a non-local one.

	How it works:
		- The client calls a process on the server to execute the code of the proceedure.
		- Arguments for the proceedure are sent in the message.
		- The result is sent in the reply.
		- RPC can be both synchronous and asynchronous as required

	- RPC is a middleware layer and acts at a high level, just below the application services level.
	- RPC must have an agreed format of messages, you can't have the client and server both using addresses.

	Stubs:
		- Stubs are the case in the client where the arguments are put in a message, sent to the server.
		- The stub then waits for the reply message, unpacks the results and returns to the application.

	Server Stub:
		- As the process is transparent the code can be used by other code on the server to provide the service to the local system. Thus RPC messages go to a server stub code on the server machine.
		- The server stub acts very much like an unpacking queue + prepration for processing and execution. As well as repacking and returning.
	
	Parameter Marshalling:
		- Packing the parameters away into a message is known as parameter marshalling, the inverse is unmarshalling.
		- These operations need to take into account problems associated with different machines + languages in use on the network.
		- Client + Server can represent floats, big-endian/little-endian, charsets, size issues (64vs32bit int) in different ways!
		- Once we have specified the exact format of the parameters we can generate the code for the stubs.

	Keyword: IDL
		Interface Definition Language: (IDL) - A specification language utilised to describe a software components interface. A language independent way to enable communication between software components that don't share a language! 
		- This is the best way to store the format for the parameters, thus enabling the remote service to be avaliable to as many languages as possible.
		- Number of different IDLS:
			- CORBA
			- Sun
			- DCE
		- They have a similar syntax to that of an interface but they also have extra information (eg. in, out, inout)

	Uuidgen - Creates a unique identifier which is put in the header file to check that the client and server stubs are compatible!

	RMI - Remote Method Invocation
		- Included in Java
		- Marshalling is simple as it is Java to Java
		- Objects sent must be serializable
		- RMI references remote objects, whereas RPC does not!

		Implementation:
			- A client has a stub (called a proxy) for each remote class instance, this is used to marshal arguments to and unmarshall arguments from.
			- This communicates with the dispatcher for the class which forwards to the skeleton for unmarshalling.
			- The server has a dispatcher and a skeleton for each remote object
			- The dispatcher recieves the metssage, and passes it to the correct method in the skeleton.
			- The skeleton implements the methods of the remote interface, unmarshalls the arguments and invokes the corresponding method in the servant.

		rmiregistry:
			- The rmiregistry makes a remote object avaliable to clients.
			- Has a string bound to the object and the clients can interrogate the registry using the string.
			- The client must know the server machine name and the port the registry is running on.

Name and Directory Servers

	Finding a Server
	- It's not desirable practice to hardwire a machine name and port number into a client!
	- A directory server is a better idea to find the machine.
	- We can use a local daemon on the resolved machine to find the port.

	Names
	- Pure names contain no information about the item they are associated with.
	- Othernames may tell you what sort of object+where is can be found.
	- An address is an extreme example of a non-pure name.

	Name Resolution
	- A name is resolved when it is translated into data about the item.
	- Names are boudn to attributed. (ie addresses)
	- A name has a namespace/domain
	- You can compose names to make bigger ones (URL's)

	URL/URI/URN
	- URI - Uniform Resource Identifiers - Identifiers on the web, starts with http: ftp: ws: ect...
	- URL - Uniform Resource Locators - Subset of URI's which provide the location for a resources
	- URN - Uniform Resource Names - URI's which are not URL's

	Namespaces
	- Can be flat, a number/string or structured - Heirarchial
	- If it's heirarchial then each part of the name is resolved in a different context

	DNS
	- Domain Name System - Names computers across the internet.
	- Uses replication and caching but strict caching is not vital
	- Contains a huge amount of data
	- In general more than one name server needs to be used to resolve a full name.

	Namespace Distribution
	- 3 Key layers:
		- Global Layer - .com/.edu/.net - Worldwide
		- Administrational Layer - cs.com/oce.com/acm.com - Organization Level
		- Managerial Layer - whatever.cs.com - Managerial Level
	- Each client has a local name resolver, it can work either iteratively or recursively.
	- Iteratively it will contact each of the require nameservers in sequence to locate the address.
	- Recursively the client will get the name servers to pass the result up through the system and only return the result.
	- Recursive puts more burden on the name servers, but makes caching more effective. It also potentially reduces communication costs. BUT global layers only support iterative resolution.

	Zones
	- DNS data is divided into zones
	- Each zone contains attributed data for a domain, but no information about the sub-domain.
	- There are two authoritative name servers for a zone.
	- They hold the names of servers for domains and zone management data.

	Name Server vs Directory Server
	- A name server takes a name and returns one or more attributes of the named object.
	- A directory server takes attributes values and returns sets to attributes of objects with those attribute values.
	- Both are like a telephone directory.

	X.500 Directory Service
	- Invented by standards organisations
	- Collection of all entries, Directory Information Base (DIB) - Portions on different servers
	- Clients are Directory User Agents (DUA's)
	- And they retrieve a Directory Information Tree (DIT)

	LDAP
	- Lightweight Directory Access Protocol
	- Simple protocol for use with X.500
	- Allows a more simple directly lookup than X.500.
	- Widely adopted.
	- There is enough information provided to navigate the DIT, then other attributes can be obtained.

Time and Logical Clocks

	Synchronization
		There are two key issues that affect synchronization
		- Difficulty of setting the same time
		- Clock drifting

	UTC - Coordinated Universal Time
	- International atomic time, derived from clocks with atomic oscillators. Rarely drifts
	- Effectively derived from sun/stars ect
	- Based on atomic time, with occational insertion of leap seconds to keep it in time.
	- UTC is broadcast by GPS

	Computer Time
	- GPS recievers are about accurate to 1 micro second
	- Recievers from terrestrial stations are accurate to a few millisec.
	- But most computers don't have these methods of setting the time.

	Cristians Clock Synchronization
	- Time Server - Clients set the clock by measuring the round-trip time to process the request and adding half the time to the reply. This assumes that the time out is the same as the time back, this is likely to be true if the rtt is short. But may be inaccurate for longer journeys.

	Berkeley Algorithm
	- 1 Processor polls the others
	- Slaves reply with their times
	- Master estimates the times using the RTT's as before.
	- Master averages the times and eliminates any anomolies.
	- Instead of returning the correct time, the master returns the time +/- the RTT.
	- If the master fails a distributed election algorithm exists to elect one of the slaves as a replacememnt.
	- Designed primarily for intranets.

	NTP - Network Time Protocol
	- Designed for larger scale internet
	- Network of servers:
		- Primary with UTC
		- Secondard synchronized with primary
	- Can be reconfigured if either source fails, so either machine can become primary/secondary.

	NTP Synchronization
	Three key methods:
		- Multicast
			- Used on highspeed LAN's
			- Server sends time to all servers on LAN at once.
			- Each resets clocks

		- Procedure call
			- Effectively cristians algorithm
			- Server accepts request and responds with time.
			- Used when multicast is not supported or higher accuracy is needed.

		- Symmetric
			- Highest accuracy
			- Messages exchanged and data built up to improve accuracy over time.
			- Each message contains timing information until the time can be accurately pinpointed.

	Data Filtering:

		- NTP servers filter successive offset and delay values, to work out the most reliable server.
		- Each server will interact with many servers to identify the most reliable
		- Achieves accuracies of 10s of millisec over internet paths.

	Logical Time (Lamport)

		- In a single processor every event can be uniquely ordered in time using the local clock.
		- We want to be able to do this is a distributed system. As we can't use physical time!

		The initial principles are very simple, if two events happen in the same process they occur in the order given by the process. And if messages are sent between processes the act of sending happens before the act of recieving. These rules define partial ordering of events, through the "happens-before" relationship.

	Logical Clock

		- The logical clock is a monotonically increasing software counter, each process has one and uses it to timestamp events.

		Totally ordered logical clocks allow us to put the ordering of events together so there is a order between every pair of events. This is done using an ordering of process identifiers where logical clocks are thte same in different processes. This is used to control entry to critical sections of code.

	Vector Clocks

		- A vector clock in a system with n processes is an array of n integers.
		- Each process keeps it's own
		- Inter process messages contain the vector clock of the sender as a timestamp.
		- Each clock starts at 0
		- Events in the process increment the processes element in the vector clock.
		- When a process recieves a timestamp in a message. It resets each element in it's clock. This is a merge.

	+
	- We don't end up with an arbitrary order when one is not required.
	Lamports clocks do not capture casuality, which can be captured by vector clocks

	-
	Costs the extra amount of data in a timestamp

Coordination & Agreement

	Mutual Exclusion:
		- In a single machine semaphores can be used to implement mutual exclusion
		- In a distributed system we can only use clever instructions in a machine with shared memory to apply semaphores.
		- A simple solution is a central server which creates a request token on entry and releases it on exit. If it doesn't have a token it get's placed in a queue.
		- But this is a single point of failure, and has the potential to be a bottleneck on the system.

	Election:
		- We need to have a process in order to implement mutual exclusion + coordinate distributed transactions.
		- Elections are a great way for getting processes in a distributes system to cooperate.
		- Our elections need to have all processes to learn and agree with the results of the election
		- Any process must be able to initiate an election so there could be more than one occuring at the same time.

		Ring Based Election:
			- Processes are arranged in a ring, so a process know's it's owners.
			- Assume there are no failures and the system is asynchronous.
			- Every process is a non-participant, apon start the process becomes a participant and sends an election message to it's neighbour with a specific identifier.
			- The reciever compares the identifier, if it is larger than it's own it forwards the message. Otherwise if it is a participant the message is discarded, if it is not then it forwards the message but with it's own identifier, then it becomes a participant.
			- This process continues around the ring until the identifier equals that of the node it arrives at. At this point the process has been elected and it sends a message around the ring to inform all the participants.

			- This process makes it tricky for new processes to join the ring. And can be quite inefficient on large scale systems.

		Bully Algorithm
			- A process knows the identifiers of all the other processes and can communicate.
			- Three messages, election, answer (response to an election message) and coordinator (sent to notify the winner).
			- A timeout period T is specified and if a message is not responded to within the time a crash is assumed.
			- A process sends an election message to all processes higher than it's self.
			- Any that have not crashed will respond.
			- Any process that recieves a election message should respond
			- The process will then initiate it's own election to all processes with a larger identifier.
			- A process with no answering processes with a higher ID assumes it's self elected and notifies all lower processes.

			- The complexity depends on which process initiates the election process, as it cascades up the nodes! o(n^2) if lowest!!!

Fault Tolerance - Transactions

	- The aim is to tolerate faults and operate in an acceptable way when a partial failure occurs.
	- Fault tolerance and dependability are strongly related.

	Dependability:
		- Avaliablity - Probability that the system is operating correctly at any time.
		- Reliablity - Length of time the system can run continuously without failure.
		- Safety - If and when failures occur the consequences are not catastropihic for the system.
		- Maintainability - How easily a failed system can be repaired.

	Types of Failures:
		- Crashes - Server Halts!!!
		- Ommision Failures - Server fails to respond/recieve to incoming requests
		- Response Failures - The servers response is incorrect
		- Timing Failures - Server fails to respond within a certain time.
		- Arbitrary Failures - A component may produce output it should have never produced, (but may not be detected as incorrect)

	Failure Masking:
		- Physical Redundancy: Planes can fly on 3/4 engines ect
		- Time Redundancy: Action can be performed repeatedly, helpful when faults are transient + intermittent.
		- Information Redundancy: Send extra bits to allow recovery.
		- BUT Redundancy costs money!!!
		- HOWEVER - We need to make sure that any failure won't leave our system in an inconsistent (corrupted) state!

	All-or-Nothing
		- Either all operations execute or none do!
		- The sequence of operations must execute as an atomic operation.

	Isolated Execution
		- We must ensure that concurrent applications do not interfere with each other!

	Serial Executions
		- Concurrent executions do not interfere with each other if their execution is equivalent to a serial one
		- Simple solution, one transfer at a time.
		- NOT SCALEABLE

	Creating a durable solution could be only performing updates after the application has completed successfully!

	An application should NEVER violate a database's integrity constraints.

	ACID

	Atomicity
	Consistency
	Isolation
	Durability

	Simple way to write database applications, ensure that the ACID properties are met. 

	A transaction either commits or aborts.

	This is a fast way to recover from all sorts of failures, and manages concurrency.

	Implementation:

	- Concurrency control algorithms
		- These algorithms are equivalent to a serial execution: transactions have a short duration to prevent blocking of other transactions.

	- Durability
		- Recovery algorithms are put in place which allows committed transactions to be replayed and undo the performed actions.

	Concurrency Control

		- Concurrency control often uses a two-phase locking where a read-lock and a write-lock are acquired before performing their respective functions. These are called "Acquire locks"
		- Release locks occur when the transaction is done regardless of the state.

	QS:

	- Redundancy is the key to handle failured.
	- We use transactions to avoid data corruption!

Distributed Transactions:

	By performing Aquire and Release locks we are reducing the potential for concurrency even though it can be really needed in many cases.
	Additionally it is possible for this to result in deadlock.

	We can make some improvements:
		- Optimistic Concurrency Control - Transactions are allowed to proceed and we check everything at the commit transaction phase, any issues at this point will result in a aborted transaction.
		- Timestamp ordering - Each operation in a transaction is validated when it is carried out. If it cannot be validated the transaction is aborted.

	Recovery:
		Backward Recovery:
			- Bring the system from it's present state back into a previously correct state, to do this we need to keep checkpoints that we can roll back to.
		Forward Recovery:
			- Bring the system to a correct new state where it can continue to execute. Must know in advance which errors may occur so it is possible to correct them.

	Distributed Transactions:
		- Distributed transactions have more than one server involved! this can make things pretty complex

		We can use All or Nothing again:
			ALWAYS
				 All databases commit or all databases abort
				 All the databases must agree on a single outcome

		Effectively all the members of the process group perform the operation or none do!

		One Phase Commit
			To support these kind of transactions we need to appoint a coordinator (could do this using bully?)

			The client then tells the coordinator to commit or abort the transaction, and this result is communicated to all the participants.
			BUT if one of the participants can't perform the operation it can't tell the coordinator.

		Two Phase Commit
			The two phase commit works very similarly except it performs a check with the participants to enquire if they can commit before sending the command to commit. It also requires a response to confirm the commit.

			BUT the two phase commit can fail, if the coordinator fails or can't be trusted. It's also possible for distributed deadlocks to occur!!!

Brucey Bonus - Deadlocks:

	If we use locking we can get deadlocks therefore we need deadlock detection!

	Deadlock Detection:
		We can resolve a deadlock by aborting one of the transactions.
		We can use priority to decide which transaction should be aborted.

	Distributed Deadlock:
		A single server is very simple to manage deadlock.
		Distributed transactions however are held in different servers and the deadlock may not be apparent to any one server.

		A solution to this is to have a coordinator which is sent the "wait-for-graph"

		But we don't really want a centralised coordination.... It may cause issues of phantom deadlocks...

	Phantom Deadlocks:

		- Information gathered is likely to be out of date
		- A transaction may have released a lock, and thus a deadlock that doesn't exist may have been detected!!!

	An alternative to centralised coordination is edge chasing/path pushing, this effectively sends a message around the servers to determine if the item is still blocked ahead. If it loops around deadlock can be detected.


Byzantine Fault Tolerance:

	- It's possible message will be lost due to an unreliable communication channel
	- It's possible that parts of a distributed system may give wrong answers.

	The Two Generals Problem
	- Two armies with a general, each on either side of the valley.
	- Must attack at the same time
	- Generals can only communicate using messengers through the valley.
	- Generals must attack at the same time to succeed!

	Solutions:
	- If we send 1000 messages the probability of being lost is low.
	- If we send 1000 messages confirming again the probability is low.

	Three Generals, but one traitor on a reliable channel
	- It's still possible for issues to arise!!!
	- We could have a majority vote, this would outnumber the traitor. But requires the messages of each general to be sent to all generals!!!

	Byzantine Failures
	- These are the worst possible type of failure
	- Handling these is the handling of byzantine fault tolerance!

	Best Practice:
	- These kind of failures may be expensive BUT they may also be rare!!!
	- Use redundancy to achieve reliablity.
	- Use majority voting to achieve reliablity.

Performance Issues:

	Performance Modelling is quite important to analyse the behaviour of a system under various circumstances.

	Performance Tuning is a huge territory and trying to find the sources and solutions to performance problems (bottlenecks)

	Approaches to Modelling:
		- Analytical
			- Formulas, Maths, Queueing Theory

		- Simulation
			- Monte-Carlo Simulations (random numbers) - Great for computing a result over a large number of samples
			- Discrete Event Simulation (rng  + events over time) - Implies a notion of time (computer games)

		Amdahls Law!! - Estimate the running time!

		Some simulations are easy and based on probability theory. We can sample the processing rate and study systems behaviour with slightly randomized parameters.

	Littles Law:

		AVG NO CUSTOMERS = arrive_rate * service_time
		simples!

Replication Reasons

	Relibility:

	Replication is a technique to increase avaliablity, if a server crashes there is a replica which can still be used. Therefore failures are tolerated by using the redundant component. ie - should always be two routes between two routers.

	Basically, the chances of all the replicated services being down at the same time is basically none...

	We just times the probabilities by each other to get the failure of all of them.

	Performance:

	By having a copy of the data near to the process using it, the access time decreases. This helps us achieve scalability, and you could say caching is an application of this!!!

	Capacity Planning:

	We need to determine the number of replicas to make to meet a level of demand! This can be done through simulation and queueing theory.

	Issues with replication:
		- Consistency issues - If copies are modified all the time we need to ensure that the copy is updated throughout the network, we have to propagate the update to all the other replicas before they can read!
		- Global Sync taks a long time if the replicas are worldwide!
		- Costing duh

	We can solve the sync issues by relaxing consistency restraints so that copies don't have to be the same everywhere. But we need to apply an update pattern to ensure that the data can eventually be brought inline.

Consistency Models

	A consistency model is a contract between processes and the data store. It acts like a contract between processes and the data store, so if certain rules are obeyed then the store will work correctly!

	It can be possible for it being hard to define what happened last if there is no global clock! 

	Tight Consistency
	Having a tight consistency means that a data item will retun the most recent write. But this is almost impossible in a distributed system as it needs absolute global time. Something that is almost impossible!!!

	Sequantial Consistency
	The result of execution is the same if they are executed in the order that they are specified by the program.

	Casual Consistency
	A casual store needs to see processes which are casually related in the same order, concurrent writes may be seen in a different order by differeny processes.

Placing Replicas

	Heuristically speaking the best place for replicas is to find the total cost of accessing each site from all the other sites. Thus finding the site with the minimum total cost.



